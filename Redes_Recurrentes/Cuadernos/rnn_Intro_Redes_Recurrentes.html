
<!DOCTYPE html>

<html lang="es">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Introducción a Redes Neuronales Recurrentes &#8212; Fundamentos de IA y AP</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../_static/translations.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Índice" href="../../genindex.html" />
    <link rel="search" title="Búsqueda" href="../../search.html" />
    <link rel="next" title="Redes LSTM (Long Short Term Memory Networks)" href="rnr_LSTM_Intro.html" />
    <link rel="prev" title="Introducción a Modelos Secuenciales" href="rnr_Times_series_Intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="es">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo-final-ap.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Fundamentos de IA y AP</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../tutorial.html">
                    <span style="color:#F72585">Bienvenido(a)</span>
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Conociendo el libro
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Inicio/Cuadernos/Consideraciones.html">
   <span style="color:#F72585">
    Conociendo el Libro
   </span>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Fundamentos de Estadística
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Estadistica/Cuadernos/Prob_Conceptos_Basicos.html">
   <span style="color:#F72585">
    Probabilidad
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Estadistica/Cuadernos/Prob_Variables_Aleatorias.html">
   <span style="color:#F72585">
    Variables Aleatorias
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Estadistica/Cuadernos/Prob_Var_Prob_conjunta.html">
   <span style="color:#F72585">
    Probabilidad Conjunta y Entropía Cruzada
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Estadistica/Cuadernos/Prob_Distribuciones_continuas.html">
   <span style="color:#F72585">
    Distribuciones de probabilidad continuas
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Estadistica/Cuadernos/Regresi%C3%B3n-Lineal-Pyton-Copy1.html">
   <span style="color:#F72585">
    Regresión Lineal en Python
   </span>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Teoría de la Información
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Estadistica/Cuadernos/ti_Teoria_Informacion.html">
   <span style="color:#F72585">
    Teoría de la Información
   </span>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Álgebra Lineal
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Matematica/Cuadernos/Intro_Tensores_I.html">
   <span style="color:#F72585">
    Introducción a tensores
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Matematica/Cuadernos/Intro_Tensores_II.html">
   <span style="color:#F72585">
    Tensores
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Matematica/Cuadernos/Tensor_Distribucion_Prob.html">
   <span style="color:#F72585">
    Tensores y distribuciones de probabilidad
   </span>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Modelación
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Matematica/Cuadernos/mod_Modelamiento.html">
   <span style="color:#F72585">
    Ejemplos de Modelos
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Matematica/Cuadernos/mod_Ejemplo_Modelamiento.html">
   <span style="color:#F72585">
    Ejemplos de Modelamiento
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Matematica/Cuadernos/cal_derivadas.html">
   <span style="color:#F72585">
    Introducción a la Derivación
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Matematica/Cuadernos/Optimization_1.html">
   <span style="color:#F72585">
    Optimización univariada usando JAX
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Matematica/Cuadernos/Optimization_2.html">
   <span style="color:#F72585">
    Optimización multivariada usando JAX
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Fundamentacion_Matematica/Cuadernos/am-sdg.html">
   <span style="color:#F72585">
    Optimización
   </span>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Aprendizaje de máquinas
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/am_intro_aprendizaje_maquinas.html">
   <span style="color:#F72585">
    Conceptos básicos de aprendizaje de máquinas
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/am_Regresion_logistica_JAX.html">
   <span style="color:#F72585">
    Modelo Lineal de Clasificación  con JAX
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/am_Regresion_Logistica_Tensorflow.html">
   <span style="color:#F72585">
    Modelo Logístico de Clasificación  con Tensorflow 2.X
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/am_regresion_Keras.html">
   <span style="color:#F72585">
    Regresion Basica con tf.keras: Predecir eficiencia de la gasolina
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/am-logistico-keras-cancer.html">
   <span style="color:#F72585">
    Modelo logístico de predicción en tf.keras
   </span>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Aprendizaje de máquinas no supervisado
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/BreveIntroduccion2R.html">
   <span style="color:#F72585">
    Breve introducción a R
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/AprendizajeNoSupervisado.html">
   <span style="color:#F72585">
    Aprendizaje no supervisado
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/ACP.html">
   <span style="color:#F72585">
    Análisis en componentes principales
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/ACS.html">
   <span style="color:#F72585">
    Análisis de correspondencias simples
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/ACM.html">
   <span style="color:#F72585">
    Análisis de correspondencias múltiples (ACM)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/Agrupamiento.html">
   <span style="color:#F72585">
    Algunos métodos de clasificación no supervisada (agrupamiento)
   </span>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Mapas auto-organizados (SOM)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Machine_Learning/Cuadernos/som_Introduccion.html">
   <span style="color:#F72585">
    Mapas Auto-organizados (SOM)
   </span>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Redes Neuronales
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Redes_Neuronales/Cuadernos/RedesNeuronales_intro.html">
   <span style="color:#F72585">
    Introducción a Redes Neuronales
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Redes_Neuronales/Cuadernos/Activation_Functions.html">
   <span style="color:#F72585">
    Funciones de Activación
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Redes_Neuronales/Cuadernos/Hello_World_ML.html">
   <span style="color:#F72585">
    Introducción a Keras Sequential y API Funcional
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Redes_Neuronales/Cuadernos/Intro_Keras_Sequential.html">
   <span style="color:#F72585">
    Introducción a la API Sequential de Keras
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Redes_Neuronales/Cuadernos/Intro_Keras_Functional.html">
   <span style="color:#F72585">
    Introducción a la API funcional de Keras
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Redes_Neuronales/Cuadernos/am-softmax-keras-iris.html">
   <span style="color:#F72585">
    Clasificación, Softmax, Iris
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Redes_Neuronales/Cuadernos/am_regresion_Keras_gasolina.html">
   <span style="color:#F72585">
    Regresion Basica con tf.keras: Predecir eficiencia de la gasolina
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Redes_Neuronales/Cuadernos/am-subclassing-iris.html">
   <span style="color:#F72585">
    Subclassing-Modelo de Regresión multi-logística
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Redes_Neuronales/Cuadernos/NN_Animation2.html">
   <span style="color:#F72585">
    Visualización del Entrenamiento de una Red Neuronal
   </span>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Redes Recurrentes
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="rnr_Times_series_Intro.html">
   <span style="color:#F72585">
    Introducción a Modelos Secuenciales
   </span>
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   <span style="color:#F72585">
    Introducción a Redes Neuronales Recurrentes
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rnr_LSTM_Intro.html">
   <span style="color:#F72585">
    Redes LSTM (Long Short Term Memory Networks)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rnr_GRU_Intro.html">
   <span style="color:#F72585">
    Redes GRU (Gated Recurrent Unit)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rnr_Times_series_Intro-lstm.html">
   <span style="color:#F72585">
    Ejemplo 1: Serie de tiempo simulada
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rnr_accion_Apple_Prediccion_un_dia-dropout.html">
   <span style="color:#F72585">
    Predicción del valor de una acción a un día. Apple (LSTM)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rnr_accion_Apple_Prediccion_tres_dias-dropout.html">
   <span style="color:#F72585">
    Predicción del valor de una acción a tres días. Apple (LSTM)
   </span>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="rnr_accion_Apple_Prediccion_tres_dias-multiple.html">
   <span style="color:#F72585">
    Predicción de secuencia de valores futuros de una acción. Apple (LSTM)
   </span>
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/AprendizajeProfundo/Libro-Fundamentos/main?urlpath=tree/Redes_Recurrentes/Cuadernos/rnn_Intro_Redes_Recurrentes.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/AprendizajeProfundo/Libro-Fundamentos/blob/main/Redes_Recurrentes/Cuadernos/rnn_Intro_Redes_Recurrentes.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
      <li>
        
<button onclick="initThebeSBT()"
  class="headerbtn headerbtn-launch-thebe"
  data-toggle="tooltip"
data-placement="left"
title="Launch Thebe"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-play"></i>
  </span>
<span class="headerbtn__text-container">Live Code</span>
</button>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/AprendizajeProfundo/Libro-Fundamentos"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/AprendizajeProfundo/Libro-Fundamentos/issues/new?title=Issue%20on%20page%20%2FRedes_Recurrentes/Cuadernos/rnn_Intro_Redes_Recurrentes.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/AprendizajeProfundo/Libro-Fundamentos/edit/main/Redes_Recurrentes/Cuadernos/rnn_Intro_Redes_Recurrentes.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Edit this page"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="headerbtn__text-container">suggest edit</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/Redes_Recurrentes/Cuadernos/rnn_Intro_Redes_Recurrentes.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-introduccion-span">
   <span style="color:#4361EE">
    Introducción
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-redes-neuronales-con-una-capa-oculta-span">
   <span style="color:#4361EE">
    Redes Neuronales con una capa oculta
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-redes-neuronales-con-estados-ocultos-span">
   <span style="color:#4361EE">
    Redes Neuronales con estados ocultos
   </span>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#span-style-color-4cc9f0-calculo-del-estado-oculto-span">
     <span style="color:#4CC9F0">
      Cálculo del estado oculto
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#span-style-color-4cc9f0-calculo-de-la-salida-en-la-capa-de-salida-para-el-tiempo-t-span">
     <span style="color:#4CC9F0">
      Cálculo de la salida en la capa de salida para el tiempo
      <span class="math notranslate nohighlight">
       \(t\)
      </span>
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-redes-neuronales-basados-en-modelos-de-lenguaje-caracter-etiqueta-span">
   <span style="color:#4361EE">
    Redes Neuronales basados en Modelos de lenguaje caracter-etiqueta
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-perplexity-span">
   <span style="color:#4361EE">
    Perplexity
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-resumen-span">
   <span style="color:#4361EE">
    Resumen
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-ejercicios-span">
   <span style="color:#4361EE">
    Ejercicios
   </span>
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1><span style="color:#F72585">Introducción a Redes Neuronales Recurrentes</span></h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-introduccion-span">
   <span style="color:#4361EE">
    Introducción
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-redes-neuronales-con-una-capa-oculta-span">
   <span style="color:#4361EE">
    Redes Neuronales con una capa oculta
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-redes-neuronales-con-estados-ocultos-span">
   <span style="color:#4361EE">
    Redes Neuronales con estados ocultos
   </span>
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#span-style-color-4cc9f0-calculo-del-estado-oculto-span">
     <span style="color:#4CC9F0">
      Cálculo del estado oculto
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#span-style-color-4cc9f0-calculo-de-la-salida-en-la-capa-de-salida-para-el-tiempo-t-span">
     <span style="color:#4CC9F0">
      Cálculo de la salida en la capa de salida para el tiempo
      <span class="math notranslate nohighlight">
       \(t\)
      </span>
     </span>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-redes-neuronales-basados-en-modelos-de-lenguaje-caracter-etiqueta-span">
   <span style="color:#4361EE">
    Redes Neuronales basados en Modelos de lenguaje caracter-etiqueta
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-perplexity-span">
   <span style="color:#4361EE">
    Perplexity
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-resumen-span">
   <span style="color:#4361EE">
    Resumen
   </span>
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#span-style-color-4361ee-ejercicios-span">
   <span style="color:#4361EE">
    Ejercicios
   </span>
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="span-style-color-f72585-introduccion-a-redes-neuronales-recurrentes-span">
<h1><span style="color:#F72585">Introducción a Redes Neuronales Recurrentes</span><a class="headerlink" href="#span-style-color-f72585-introduccion-a-redes-neuronales-recurrentes-span" title="Enlazar permanentemente con este título">#</a></h1>
<p>Conceptos básicos</p>
<figure>
<img src="https://raw.githubusercontent.com/AprendizajeProfundo/Libro-Fundamentos/main/Redes_Recurrentes/Imagenes/recurrente_char.png" width="700" height="300" align="center"/>
</figure>
<p><strong><center >Modelo PLN generador de texto a nivel de caracter</center></strong></p>
<p>Fuente: Alvaro Montenegro</p>
<section id="span-style-color-4361ee-introduccion-span">
<h2><span style="color:#4361EE">Introducción</span><a class="headerlink" href="#span-style-color-4361ee-introduccion-span" title="Enlazar permanentemente con este título">#</a></h2>
<p>En la introducción a procesamiento de lenguaje natural introducimos los modelos <span class="math notranslate nohighlight">\( n \)</span>-gram. En tales modelos la probabilidad condicional de la palabra <span class="math notranslate nohighlight">\( x_t \)</span> en el paso de tiempo <span class="math notranslate nohighlight">\( t \)</span> solo depende de <span class="math notranslate nohighlight">\( n-1 \)</span> las palabras anteriores .</p>
<p>Si queremos incorporar el posible efecto de palabras anteriores al paso de tiempo <span class="math notranslate nohighlight">\( t-(n-1) \)</span> en <span class="math notranslate nohighlight">\( x_t \)</span>,
necesitamos aumentar <span class="math notranslate nohighlight">\( n \)</span>.</p>
<p>Sin embargo, el número de parámetros del modelo también aumentaría exponencialmente con él, ya que necesitamos almacenar <span class="math notranslate nohighlight">\( | \mathcal{V} |^n \)</span> números para un conjunto de vocabulario <span class="math notranslate nohighlight">\( \mathcal{V} \)</span>.
Por lo tanto, en lugar de modelar <span class="math notranslate nohighlight">\( P (x_t \mid x_ {t-1}, \ldots, x_{t-n + 1}) \)</span>, es preferible usar un modelo de variable latente:</p>
<div class="math notranslate nohighlight">
\[ 
P(x_t \mid x_ {t-1}, \ldots, x_1) \approx P (x_t \mid h_ {t-1}), 
\]</div>
<p>en donde <span class="math notranslate nohighlight">\( h_{t-1} \)</span> es un <code class="docutils literal notranslate"><span class="pre">estado</span> <span class="pre">oculto</span></code> (también conocido como variable oculta) que almacena la información de la secuencia hasta el paso de tiempo <span class="math notranslate nohighlight">\( t-1 \)</span>.</p>
<p>En general, el estado oculto en cualquier paso de tiempo <span class="math notranslate nohighlight">\( t \)</span> podría calcularse basándose tanto en la entrada actual <span class="math notranslate nohighlight">\( x_{t} \)</span> como en el estado oculto anterior <span class="math notranslate nohighlight">\( h_{t-1} \)</span>:</p>
<div class="math notranslate nohighlight">
\[
h_t = f (x_{t}, h_{t-1}).
\]</div>
<p>Para una función suficientemente poderosa <span class="math notranslate nohighlight">\( f \)</span>, el modelo de variable latente no es una aproximación. Después de todo, <span class="math notranslate nohighlight">\( h_t \)</span> simplemente puede almacenar todos los datos que ha observado hasta ahora.
Sin embargo, potencialmente podría encarecer tanto la computación como el almacenamiento.</p>
<p>Recuerde que hemos hablado de capas ocultas con unidades ocultas cuando introducimos los  perceptrones multicapa.</p>
<p>Cabe destacar que capas ocultas y estados ocultos se refieren a dos conceptos muy diferentes.</p>
<ul class="simple">
<li><p>Las capas ocultas son, como se explicó, capas que están ocultas a la vista en la ruta desde la entrada hasta la salida.</p></li>
<li><p>Los estados ocultos son técnicamente hablando <code class="docutils literal notranslate"><span class="pre">entradas</span></code> a todo lo que hacemos en un paso dado, y solo se pueden calcular observando los datos de los pasos de tiempo anteriores.</p></li>
</ul>
<p>Las <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">neuronales</span> <span class="pre">recurrentes</span></code> (RNR) son redes neuronales con estados ocultos. Antes de presentar el modelo RNN, revisitemos el modelo perceptron multicapa (PMC).</p>
</section>
<section id="span-style-color-4361ee-redes-neuronales-con-una-capa-oculta-span">
<h2><span style="color:#4361EE">Redes Neuronales con una capa oculta</span><a class="headerlink" href="#span-style-color-4361ee-redes-neuronales-con-una-capa-oculta-span" title="Enlazar permanentemente con este título">#</a></h2>
<p>Damos un vistazo a un PMC con una sola capa oculta. Dado un mini lote de ejemplos <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{d \times N}\)</span> con tamaño de lote <span class="math notranslate nohighlight">\(N\)</span> y <span class="math notranslate nohighlight">\(d\)</span> entradas. La salida de la capa oculta, denotada por  <span class="math notranslate nohighlight">\(\mathbf{Y} \in \mathbb{R}^{h \times N}\)</span> se calcula como</p>
<div class="math notranslate nohighlight">
\[\mathbf{Y} = g( \mathbf{W}_{hd} \mathbf{X}+ \mathbf{B})\]</div>
<p>En esta ecuación tenemos que la matriz(parámetro) de pesos de la capa oculta es <span class="math notranslate nohighlight">\(\mathbf{W}_{hd} \in \mathbb{R}^{d \times h}\)</span>, el vector sesgo (bias) de la capa oculta es  <span class="math notranslate nohighlight">\(\mathbf{b}_h \in \mathbb{R}^{h\times 1}\)</span>, <span class="math notranslate nohighlight">\(B = [\mathbf{b}_h, \ldots, \mathbf{b}_h ]_{h \times N}\)</span>, <span class="math notranslate nohighlight">\(h\)</span>  es el número de unidades ocultas de la capa oculta y <span class="math notranslate nohighlight">\(g\)</span> la función de activación de la capa oculta.</p>
<p>Ahora, la variable oculta <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> se utiliza como entrada de la capa de salida. La capa de salida está dada por</p>
<div class="math notranslate nohighlight">
\[\mathbf{Z} = f(\mathbf{V}_{qh}\mathbf{Y} + \mathbf{C}),\]</div>
<p>en donde <span class="math notranslate nohighlight">\(\mathbf{Z} \in \mathbf{R}^{q \times N}\)</span> es la variable de salida, <span class="math notranslate nohighlight">\(\mathbf{V}_{q \times h} \in \mathbb{R}^{q \times h}\)</span> es la matriz (parámetro) de pesos de la capa de salida, <span class="math notranslate nohighlight">\(\mathbf{C} \in \mathbb{R}^{q \times N}\)</span>, con
<span class="math notranslate nohighlight">\(\mathbf{C} = [\mathbf{c}_q, \ldots, \mathbf{c}_q ]_{q \times N}\)</span>,  <span class="math notranslate nohighlight">\(\mathbf{c}_q \in \mathbb{R}^{q \times 1}\)</span> el parámetro de sesgo de la capa de salida y <span class="math notranslate nohighlight">\(f\)</span> la función de activación de la capa de salida.  Si se trata de un problema de clasificación, podemos utilizar <span class="math notranslate nohighlight">\(\text{softmax}(\mathbf{Z})\)</span> para calcular la distribución de probabilidad de las categorías de salida.</p>
<p>Esto es completamente análogo a un problema de regresión no lineal usando redes neuronales, por lo que omitimos detalles.
Baste decir que podemos elegir pares de características y etiquetas al azar y <code class="docutils literal notranslate"><span class="pre">aprender</span></code> (estimar) los parámetros de nuestra red a través de la <code class="docutils literal notranslate"><span class="pre">diferenciación</span> <span class="pre">automática</span></code> y el <code class="docutils literal notranslate"><span class="pre">descenso</span> <span class="pre">de</span> <span class="pre">gradiente</span> <span class="pre">estocástico</span></code>.</p>
<p>La siguiente imagen ilustra el caso cuando se tiene <span class="math notranslate nohighlight">\(d=3\)</span>, <span class="math notranslate nohighlight">\(h=2\)</span>, <span class="math notranslate nohighlight">\(q=1\)</span> y <span class="math notranslate nohighlight">\(N=1\)</span>.</p>
<figure>
<center>
<img src="https://raw.githubusercontent.com/AprendizajeProfundo/Libro-Fundamentos/main/Redes_Recurrentes/Imagenes/ANN_Capa_Oculta.png" width="700" height="400" align="center"/>
</center>
</figure>
<p><strong><center >Red Neuronal con una capa oculta.</center></strong>
Fuente: Alvaro Montenegro</p>
</section>
<section id="span-style-color-4361ee-redes-neuronales-con-estados-ocultos-span">
<h2><span style="color:#4361EE">Redes Neuronales con estados ocultos</span><a class="headerlink" href="#span-style-color-4361ee-redes-neuronales-con-estados-ocultos-span" title="Enlazar permanentemente con este título">#</a></h2>
<p>Las cosas son completamente diferentes cuando tenemos estados ocultos. Veamos la estructura con más detalle.</p>
<p>Supongamos que tenemos un minilote de entradas <span class="math notranslate nohighlight">\( \mathbf{X}^{(t)} \in \mathbb{R} ^ {d \times N} \)</span> en el paso de tiempo <span class="math notranslate nohighlight">\( t \)</span>. En otras palabras, para un minilote de <span class="math notranslate nohighlight">\( N \)</span> ejemplos de secuencia,
cada columna de <span class="math notranslate nohighlight">\( \mathbf{X}^{(t)} \)</span> corresponde a un ejemplo en el paso de tiempo <span class="math notranslate nohighlight">\( t \)</span> de la secuencia.</p>
<p>En adelante  <span class="math notranslate nohighlight">\(\mathbf{H}^{(t)} \in \mathbb{R}^{h \times N} \)</span> denotará la variable oculta en el  paso de tiempo <span class="math notranslate nohighlight">\( t \)</span>.</p>
<p>A diferencia del PMC, aquí guardamos la variable oculta <span class="math notranslate nohighlight">\( \mathbf {H}^{(t-1)} \)</span> del paso de tiempo anterior e introducimos una nueva matriz (parámetro) de peso <span class="math notranslate nohighlight">\( \mathbf{W}_{hh} \in \mathbb{R }^{h \times h}\)</span> para transformar  la variable oculta del paso de tiempo anterior en el paso de tiempo actual.  En adelante <span class="math notranslate nohighlight">\( \mathbf{W}_{hd} \in \mathbb{R }^{h \times d}\)</span>  denotará a la matriz de pesos aplicada a la siguiente nueva entrada <span class="math notranslate nohighlight">\( \mathbf {X}^{(t)} \)</span></p>
<section id="span-style-color-4cc9f0-calculo-del-estado-oculto-span">
<h3><span style="color:#4CC9F0">Cálculo del estado oculto</span><a class="headerlink" href="#span-style-color-4cc9f0-calculo-del-estado-oculto-span" title="Enlazar permanentemente con este título">#</a></h3>
<p>Específicamente, el cálculo de la variable oculta del paso de tiempo actual está determinado por la entrada del paso de tiempo actual junto con la variable oculta del paso de tiempo anterior:</p>
<div class="math notranslate nohighlight">
\[ 
\mathbf{H}^{(t)} = f( \mathbf{W}_{h d} \mathbf{X}^{(t)} + \mathbf{W}_{hh} \mathbf{H}^{(t-1)}  + \mathbf{B}). 
\]</div>
<p>En donde <span class="math notranslate nohighlight">\(\mathbf{b}_h \in \mathbb{R}^{h\times 1}\)</span>, <span class="math notranslate nohighlight">\(B = [\mathbf{b}_h, \ldots, \mathbf{b}_h ]_{h \times N}\)</span></p>
<p>En comparación con la ecuación de la salida de la capa oculta arriba, esta ecuación   agrega el término <span class="math notranslate nohighlight">\( \mathbf {W}_{hh}\mathbf{H}^{(t-1)} \)</span>.</p>
<p>De la relación entre las variables ocultas <span class="math notranslate nohighlight">\( \mathbf{H}^{(t)} \)</span> y <span class="math notranslate nohighlight">\( \mathbf{H}^{(t-1)}\)</span> de pasos de tiempo adyacentes, se desprende que estas variables capturan y retienen la información histórica de la secuencia hasta su paso de tiempo actual.</p>
<p>La variable oculta se denomina <code class="docutils literal notranslate"><span class="pre">estado</span> <span class="pre">oculto</span></code>. Dado que el estado oculto usa la misma definición del paso de tiempo anterior en el paso de tiempo actual, el cálculo de <span class="math notranslate nohighlight">\( \mathbf {H}^{(t)}\)</span> es recurrente.  Por esta razón las redes neuronales con estados ocultos basados en cálculos recurrentes se denominan <code class="docutils literal notranslate"><span class="pre">redes</span> <span class="pre">neuronales</span> <span class="pre">recurrentes</span></code>.</p>
<p>Capas que ejecutan el cálculo de esta última ecuación se llaman <em>capas recurrentes</em>.</p>
<p>Hay muchas formas diferentes de construir RNR. Las RNR con un estado oculto definido arriba  son muy comunes.</p>
</section>
<section id="span-style-color-4cc9f0-calculo-de-la-salida-en-la-capa-de-salida-para-el-tiempo-t-span">
<h3><span style="color:#4CC9F0">Cálculo de la salida en la capa de salida para el tiempo <span class="math notranslate nohighlight">\(t\)</span></span><a class="headerlink" href="#span-style-color-4cc9f0-calculo-de-la-salida-en-la-capa-de-salida-para-el-tiempo-t-span" title="Enlazar permanentemente con este título">#</a></h3>
<p>Para el paso de tiempo <span class="math notranslate nohighlight">\( t \)</span>, la salida de la capa de salida es similar al cálculo en el PMC:</p>
<div class="math notranslate nohighlight">
\[\mathbf{Z}^{(t)} =  \mathbf {W}_{qh} \mathbf {H}^{(t)}+ \mathbf {b}_q. \]</div>
<p>Los parámetros de la RNR incluyen los pesos <span class="math notranslate nohighlight">\( \mathbf{W}_{dh} \in \mathbb{R}^{d \times h}, \mathbf{W}_{hh} \in \mathbb{R}^{h \times q}\)</span> y el sesgo <span class="math notranslate nohighlight">\( \mathbf{b}_h \in \mathbb{R}^{1 \times h} \)</span>
de la capa oculta, junto con los pesos <span class="math notranslate nohighlight">\( \mathbf {W} _ {hq} \in \mathbb {R} ^ {h \times q} \)</span>
y el sesgo <span class="math notranslate nohighlight">\( \mathbf{b}_q \in \mathbb{R}^{1 \times q} \)</span> de la capa de salida.</p>
<p>Vale la pena mencionar que incluso en diferentes pasos de tiempo, Los RNR siempre usan estos parámetros de modelo. Por tanto, el costo de parametrización de un RNR no crece a medida que aumenta el número de pasos de tiempo.</p>
<p>La siguiente figura ilustra la lógica computacional de un RNR en tres pasos de tiempo adyacentes.
En cualquier momento paso <span class="math notranslate nohighlight">\( t \)</span>, el cálculo del estado oculto se puede entender como:</p>
<ol class="simple">
<li><p>concatenando la entrada <span class="math notranslate nohighlight">\( \mathbf {X}^{(t)} \)</span> en el paso de tiempo actual <span class="math notranslate nohighlight">\( t \)</span> y el estado oculto <span class="math notranslate nohighlight">\( \mathbf {H}^{(t-1)} \)</span> en el paso de tiempo anterior <span class="math notranslate nohighlight">\( t-1 \)</span>;</p></li>
<li><p>pasar el resultado de la concatenación a una capa completamente conectada con función de activación <span class="math notranslate nohighlight">\( f \)</span>.</p></li>
</ol>
<figure>
<center>
<img src="https://raw.githubusercontent.com/AprendizajeProfundo/Libro-Fundamentos/main/Redes_Recurrentes/Imagenes/red_recurrente.png" width="700" height="300" align="center"/>
</center>
</figure>
<p><strong><center >Red Neuronal recurrente con un estado oculto.</center></strong></p>
<p>Fuente: Alvaro Montenegro</p>
</section>
</section>
<section id="span-style-color-4361ee-redes-neuronales-basados-en-modelos-de-lenguaje-caracter-etiqueta-span">
<h2><span style="color:#4361EE">Redes Neuronales basados en Modelos de lenguaje caracter-etiqueta</span><a class="headerlink" href="#span-style-color-4361ee-redes-neuronales-basados-en-modelos-de-lenguaje-caracter-etiqueta-span" title="Enlazar permanentemente con este título">#</a></h2>
<p>Recuerde que para el modelado de idiomas nuestro objetivo es predecir el siguiente token basado en
los tokens actuales y anteriores, Así cambiamos la secuencia original por un token.
como las etiquetas. <a class="reference external" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">Bengio et al</a>. fueron los primeros en proponer usar una red neuronal para el modelado de idiomas.</p>
<p>A continuación ilustramos cómo se pueden usar RNN para predecier el siguiente caracter en un modelo de idioma basado en caracteres. Dejaremos que el tamaño del minilote sea uno, y la secuencia del texto sea «machine».</p>
<p>Para simplificar, tokenizamos el texto en caracteres en lugar de las palabras y consideramos un <code class="docutils literal notranslate"><span class="pre">modelo</span> <span class="pre">de</span> <span class="pre">lenguaje</span> <span class="pre">de</span> <span class="pre">nivel</span> <span class="pre">de</span> <span class="pre">carácter</span></code>.
La siguiente figura muestra cómo predecir el siguiente carácter en función de los caracteres actuales y anteriores a través de una RNR para el modelado de lenguaje de nivel de carácter.</p>
<figure>
<center>
<img src="https://raw.githubusercontent.com/AprendizajeProfundo/Libro-Fundamentos/main/Redes_Recurrentes/Imagenes/recurrente_char.png" width="700" height="300" align="center"/>
</center>
</figure>
<p><strong><center >Modelo PLN generador de texto a nivel de caracter</center></strong></p>
<p>Fuente: Alvaro Montenegro</p>
<p>Durante el proceso de entrenamiento,
Ejecuamos una operación <code class="docutils literal notranslate"><span class="pre">SoftMax</span></code> en la salida de la capa de salida para cada paso de tiempo, y luego usamos la pérdida entropía cruzada para calcular el error entre la salida del modelo y la etiqueta.</p>
<p>Debido al cálculo recurrente del estado oculto en la capa oculta, la salida del tiempo en el paso 3 en la figura, es <span class="math notranslate nohighlight">\( \mathbf{Z} _3\)</span>, y está determinado por la secuencia de texto «h», «o», y «l», . Dado que el siguiente carácter de la secuencia en los datos de entrenamiento es «a», la pérdida en el tiempo  o paso 3 dependerá de la distribución de probabilidad del siguiente carácter generado en función de la secuencia de características «h», «o», «l» y la etiqueta «a» de este paso de tiempo.</p>
<p>En la práctica, cada token está representado por un vector <span class="math notranslate nohighlight">\( d \)</span>-dimensional, y usamos un tamaño de lote <span class="math notranslate nohighlight">\( n&gt; 1 \)</span>. Por lo tanto, la entrada <span class="math notranslate nohighlight">\( \mathbf{X}^{(t)} \)</span> en el momento <span class="math notranslate nohighlight">\( t \)</span> será una matriz de <span class="math notranslate nohighlight">\( d \times N \)</span>, que es idéntica a la que discutimos antes.</p>
</section>
<section id="span-style-color-4361ee-perplexity-span">
<h2><span style="color:#4361EE">Perplexity</span><a class="headerlink" href="#span-style-color-4361ee-perplexity-span" title="Enlazar permanentemente con este título">#</a></h2>
<p>Por último, analicemos cómo medir la calidad del modelo de lenguaje, que se utilizará para evaluar nuestros modelos basados en RNR en las secciones siguientes.</p>
<p>Una forma de evaluar la calidad del modelo de lenguaje es calcular lo sorprendente que resulta el texto. Un buen modelo de lenguaje puede predecir con tokens de alta precisión como revisamos a continuación. Considere las siguientes continuaciones de la frase «Está lloviendo», según lo propuesto por diferentes modelos de lenguaje:</p>
<ol class="simple">
<li><p>«Está lloviendo afuera»</p></li>
<li><p>«Está lloviendo banano»</p></li>
<li><p>«Está lloviendo piouw; kcj pwepoiut»</p></li>
</ol>
<p>En términos de calidad, el ejemplo 1 es claramente el mejor. Las palabras son sensatas y lógicamente coherentes. Si bien es posible que no refleje con precisión qué palabra sigue semánticamente («en San Francisco» y «en invierno» habrían sido extensiones perfectamente razonables), el modelo es capaz de capturar qué tipo de palabra sigue.</p>
<p>El ejemplo 2 es considerablemente peor al producir una extensión sin sentido. No obstante, al menos el modelo ha aprendido a deletrear palabras y cierto grado de correlación entre las palabras. Por último, el ejemplo 3 indica un modelo mal entrenado que no se ajusta a los datos correctamente.</p>
<p>Podríamos medir la calidad del modelo calculando la probabilidad de la secuencia.
Desafortunadamente, este es un número difícil de entender y de comparar.
Después de todo, es mucho más probable que ocurran secuencias más cortas que las más largas, de ahí la evaluación del modelo de la obra magna de Tolstoi
<em>La guerra y paz</em> producirá inevitablemente una probabilidad mucho menor que, por ejemplo, en la novela de Saint-Exupery <em>El Principito</em>. Lo que falta es el equivalente a un promedio.</p>
<p>Como sabemos de antes, La  <em>entropía mide el grado de sorpresa</em> de un resultado en una distribución discreta y <em>entropía cruzada mide el grado de sorpesa de un resultado de una distribución en relación con una distribución de referencia</em>.</p>
<p>Si queremos comprimir texto, podemos preguntar sobre el siguiente token dado el conjunto actual de tokens. Un mejor modelo de lenguaje debería permitirnos predecir el próximo token con mayor precisión. Por lo tanto, debería permitirnos gastar menos bits en comprimir la secuencia.</p>
<p>Entonces podemos medirlo por la pérdida de la entropía cruzada promediada sobre todos los <span class="math notranslate nohighlight">\( n \)</span> tokens de una secuencia:</p>
<div class="math notranslate nohighlight">
\[\frac{1}{n} \sum_{t=1}^n -\log P(x_t \mid x_{t-1}, \ldots, x_1),\]</div>
<p>donde <span class="math notranslate nohighlight">\( P \)</span> viene dado por un modelo de lenguaje y <span class="math notranslate nohighlight">\( x_t \)</span> es el token real observado en el paso de tiempo <span class="math notranslate nohighlight">\( t \)</span> de la secuencia. Observe que para este cálculo se usa la entropía cruzada. La distribución de referencia es la codificación one-hot de la etiqueta correspondiente, en cada salida. Además <span class="math notranslate nohighlight">\(P(x_t \mid x_{t-1}, \ldots, x_1)\)</span> se calcula a partir de <span class="math notranslate nohighlight">\(\text{Softmax}(h_t)\)</span>.</p>
<p>Esto hace que el rendimiento en documentos de diferentes longitudes sea comparable. Por razones históricas, los científicos en el procesamiento del lenguaje natural prefieren usar una cantidad llamada <em>perplexity</em>. En pocas palabras, es el exponencial de la anterior ecuacion:</p>
<div class="math notranslate nohighlight">
\[\exp\left(-\frac{1}{n} \sum_{t=1}^n \log P(x_t \mid x_{t-1}, \ldots, x_1)\right).\]</div>
<p>La perplejidad (perplexity) puede entenderse mejor como la media armónica del número de opciones reales que tenemos al decidir qué token elegir a continuación.</p>
<p>Veamos algunos casos:</p>
<ul class="simple">
<li><p>En el mejor de los casos, el modelo siempre estima perfectamente la probabilidad del token de etiqueta como 1. En este caso, la perplejidad del modelo es 1.</p></li>
<li><p>En el peor de los casos, el modelo siempre predice la probabilidad del token de etiqueta como 0. En esta situación, la perplejidad es infinito positivo.</p></li>
<li><p>En la línea de base, el modelo predice una distribución uniforme sobre todos los tokens disponibles del vocabulario. En este caso, la perplejidad (perplexity) es igual al número de tokens únicos del vocabulario. De hecho, si tuviéramos que almacenar la secuencia sin ninguna compresión, esto sería lo mejor que podríamos hacer para codificarla. Por lo tanto, esto proporciona un límite superior no trivial que cualquier modelo útil debe superar.</p></li>
</ul>
</section>
<section id="span-style-color-4361ee-resumen-span">
<h2><span style="color:#4361EE">Resumen</span><a class="headerlink" href="#span-style-color-4361ee-resumen-span" title="Enlazar permanentemente con este título">#</a></h2>
<ul class="simple">
<li><p>Una red neuronal que utiliza computación recurrente para estados ocultos se denomina red neuronal recurrente (RNN).</p></li>
<li><p>El estado oculto de un RNN puede capturar información histórica de la secuencia hasta el paso de tiempo actual.</p></li>
<li><p>El número de parámetros del modelo RNN no aumenta a medida que aumenta el número de pasos de tiempo.</p></li>
<li><p>Podemos crear modelos de lenguaje a nivel de personaje usando un RNN.</p></li>
<li><p>Podemos utilizar la perplejidad para evaluar la calidad de los modelos lingüísticos.</p></li>
</ul>
</section>
<section id="span-style-color-4361ee-ejercicios-span">
<h2><span style="color:#4361EE">Ejercicios</span><a class="headerlink" href="#span-style-color-4361ee-ejercicios-span" title="Enlazar permanentemente con este título">#</a></h2>
<ol class="simple">
<li><p>Si usamos un RNN para predecir el siguiente carácter en una secuencia de texto, ¿cuál es la dimensión requerida para cualquier resultado?</p></li>
<li><p>¿Por qué los RNN pueden expresar la probabilidad condicional de un token en algún paso de tiempo basándose en todos los tokens anteriores en la secuencia de texto?</p></li>
<li><p>¿Qué le sucede al gradiente si se propaga hacia atrás a través de una secuencia larga?</p></li>
<li><p>¿Cuáles son algunos de los problemas asociados con el modelo de lenguaje descrito en esta sección?</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "AprendizajeProfundo/Libro-Fundamentos",
            ref: "main",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./Redes_Recurrentes\Cuadernos"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="rnr_Times_series_Intro.html" title="anterior página">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">anterior</p>
            <p class="prev-next-title"><span style="color:#F72585">Introducción a Modelos Secuenciales</span></p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="rnr_LSTM_Intro.html" title="siguiente página">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">siguiente</p>
        <p class="prev-next-title"><span style="color:#F72585">Redes LSTM (Long Short Term Memory Networks)</span></p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Álvaro Mauricio Montenegro Díaz, Daniel Mauricio Montenegro Reyes<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>