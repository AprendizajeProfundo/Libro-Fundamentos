{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <span style=\"color:#F72585\"><center>Método entropía cruzada</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d95e0eb-d692-405b-b0df-385ead144345",
   "metadata": {},
   "source": [
    "<center>Implementación</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bc449-4793-4745-af77-7306a1228453",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Aprendizaje_Reforzado/Imagenes/trainer.png\" width=\"800\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be027523",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee1cf1-9a63-4312-a3b7-50c153ff1582",
   "metadata": {},
   "source": [
    "La entropía cruzada se considera un algoritmo evolutivo: algunos individuos se muestrean de una población, y solo los de `élite` gobiernan las características de las generaciones futuras.\n",
    "\n",
    "Esencialmente, lo que hace el método de `entropía-cruzada`(cross-entropy) es tomar un montón de entradas, ver las salidas producidas, elegir las entradas que han llevado a las mejores salidas y ajustar el agente hasta que estemos satisfechos con las salidas que vemos.\n",
    "\n",
    "Antes de describir el método técnicamente vamos a repasar los conceptos básicos del aprendizaje reforzado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679b4575-2a33-434c-b8d0-6fe0132a6d28",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Aprendizaje_Reforzado/Imagenes/agente-ambiente.png\" width=\"700\" height=\"500\" align=\"center\"/>\n",
    "</center> \n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f0ccb4-0bd5-488c-98b2-78a98edaaf18",
   "metadata": {},
   "source": [
    "- Un conjunto de `acciones` que se permite sean ejecutadas en el ambiente.\n",
    "- El tamaño y bordes de las `observaciones` que el ambiente le provee al agente.\n",
    "- Un método *step* para ejecutar una acción. El método regresa la nueva observación, la `recompensa` y la indicación de si el `episodio` ha terminado (*done*).\n",
    "- Un método *reset* que retorna al ambiente a su *estado inicial* y entrega la primera observación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf69bef-2f6f-47c2-bdca-d032646bdf9d",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Implementación básica del Método Entropia Cruzada</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5407d82a-0cf8-47e3-b9c2-7e5559ea4693",
   "metadata": {},
   "source": [
    "Primero demos una mirada al espacio de observaciones del ambiente. El método render de un objeto de tipo Env renderiza el espacio de acciones. Para interpretar *S* es start, *F* es free, *H*  es hole y *G* es goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebd074-a57d-481b-91a7-5056bd17ce53",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">importa librerías</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3b4116-1e15-4991-9c72-07f73cf5a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, gym.spaces\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7c2a9-cd83-4d83-b920-515d1d7eb7a0",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Envuelve el espacio de acciones para el ambiente FrozenLake</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3027fd6-a44f-436f-8716-8a691141dd30",
   "metadata": {},
   "source": [
    "Con esta clase se envuelve el espacio de acciones para convertirlo en un espacio de tipo de tal manera que sea compatible con el tipo de espacio de CartPole, que vamos a estudiar en la siguiente lección. El nuevo tipo de observation será de  *Box* y contendrá un vector de tamaño 16, de tipo *onehot*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5b9337-e20a-4af2-83ee-ec3eb36de48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
    "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
    "        shape = (env.observation_space.n, )\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            0.0, 1.0, shape=shape, dtype=np.float32)\n",
    "        \n",
    "    def observation(self, observation):\n",
    "        res = np.copy(self.observation_space.low)\n",
    "        res[observation] = 1.0\n",
    "        return res\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc7bf6-5b47-4599-9d28-3c77d4fee683",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase Net</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bb5232-37ed-406c-a71c-ce4a63de3527",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class Net_basic(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net_basic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "        self.optimizer = self.configure_optimizers()\n",
    "        self.loss = self.configure_looses()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    " \n",
    "    def configure_optimizers(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def configure_looses(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def training_step(self, train_batch):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89601d28-6f72-48c2-81bc-55ea8b4f852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Net(Net_basic): \n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__(obs_size, hidden_size, n_actions)\n",
    "           \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "    \n",
    "    def configure_looses(self):\n",
    "        loss = torch.nn.CrossEntropyLoss()\n",
    "        return loss\n",
    "    \n",
    "    def training_step(self, train_batch):\n",
    "        x, y = train_batch\n",
    "        y_hat = self.net(x)\n",
    "        loss = self.loss(y_hat, y)\n",
    "        #self.log('train_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f760ebbc-db94-46ac-a9ed-7771c3eab6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, model, writer, verbose=True):\n",
    "        self.model = model\n",
    "        self.writer = writer\n",
    "        self.verbose = verbose\n",
    "        \n",
    "    def fit(self, iter_no, train_dataloader, reward_bound, reward_mean):\n",
    "        loss_l = []\n",
    "        for batch in train_dataloader:\n",
    "            self.model.optimizer.zero_grad()\n",
    "            loss = self.model.training_step(batch)\n",
    "            loss_l.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.model.optimizer.step()\n",
    "            \n",
    "        mean_loss = np.mean(loss_l)\n",
    "        \n",
    "        # escribe en el log de writer\n",
    "        self.writer.add_scalar(\"perdida\", mean_loss, iter_no)\n",
    "        self.writer.add_scalar(\"recompensa_promedio\", reward_mean, iter_no)\n",
    "        self.writer.add_scalar(\"recompensa_frontera\", reward_bound, iter_no)\n",
    "        \n",
    "        # escribe en la pantalla\n",
    "        if self.verbose:\n",
    "            print(\"%d: pérdida promedio =%.3f, recompensa promedio=%.1f, cota recompensa=%.1f\" % (\n",
    "            iter_no, mean_loss, reward_mean, reward_bound))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7ea8ee-8a23-4c6c-b74a-079f167ae858",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase iterable Batch</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1159fc6-c604-4edd-baa1-b473fb157b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "#  tupla para retornar todos los datos de un episodio completo\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "# tupla para para almacenar las parejas (observación, acción) de cada paso \n",
    "# en un episodio\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "class Batch(object):\n",
    "    '''\n",
    "    Implementa la generación iterativa de lotes de  datos\n",
    "    '''\n",
    "    def __init__(self, env, net, batch_size):\n",
    "        self.env = env\n",
    "        self.net = net\n",
    "        self.batch_size = batch_size\n",
    "   \n",
    "    # hace la clase iterable       \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    # define iterador\n",
    "    def __next__(self):\n",
    "        # lista que contendrá el lote  de datos a entregar\n",
    "        batch = [] \n",
    "        # recompensa de cada episodio\n",
    "        episode_reward = 0.0 \n",
    "        # lista de parejas (observación, acción) de cada episodio\n",
    "        episode_steps = [] \n",
    "        \n",
    "        # reinicia el ambiente, para empezar a generar datos\n",
    "        # recibe la primera observación\n",
    "        obs = self.env.reset()\n",
    "        # alias para la función softmax\n",
    "        sm = nn.Softmax(dim=1)\n",
    "        \n",
    "        # ciclo para generar lote(batch) de datos\n",
    "        while True:\n",
    "            # convierte obs a un tensor. debe pasar como lista\n",
    "            obs_v = torch.FloatTensor(np.array([obs]))\n",
    "            # calcula el tensor de puntaje para las acciones: self.net(obs_v)\n",
    "            # Transforma los puntajes entregado por la red en una distribución\n",
    "            # de probabilidad, la cual viene en un tensor\n",
    "            act_probs_v = sm(self.net(obs_v))\n",
    "            # extrae la distribución del tensor a d-array de  Numpy\n",
    "            act_probs = act_probs_v.data.numpy()[0]\n",
    "            # selecciona una acción aleatoriamente usando la distribución\n",
    "            action = np.random.choice(len(act_probs), p=act_probs)\n",
    "            # entrega la acción al ambiente y recibe respuesta del ambiete\n",
    "            next_obs, reward, is_done, _ = self.env.step(action)\n",
    "            # actualzia la recompensa\n",
    "            episode_reward += reward\n",
    "            # agrega la pareja (observación, acción) a la lista de pasos\n",
    "            episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
    "            # al terminar el episodio\n",
    "            if is_done:\n",
    "                # agrega los datos al batch: (recompensa, lista de parejas (obs, acción))\n",
    "                batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
    "                # reinicia objetos para el siguiente episodio\n",
    "                episode_reward = 0.0\n",
    "                episode_steps = []\n",
    "                next_obs = env.reset()\n",
    "                # si completo el lote de datos, lo retorna y termina\n",
    "                if len(batch) == self.batch_size:\n",
    "                    return batch\n",
    "            obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f814382f-f0df-4475-9e73-dc0c639df154",
   "metadata": {},
   "source": [
    "#### Prueba del iterador de lotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebde585c-42fe-4faa-ac0e-b1ce6907b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecciona un ambiente FrozenLake\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "\n",
    "# extrae tamaños de acciones y observaciones en el ambiente\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "# define tamaño capa oculta de la red\n",
    "HIDDEN_SIZE = 128\n",
    "#instancia un objeto Net\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "\n",
    "# define tamaño de los lotes\n",
    "BATCH_SIZE = 2\n",
    "# instancia un iterador Batch\n",
    "batch = Batch(env, net, BATCH_SIZE)\n",
    "\n",
    "# extrae el primer lote de datos\n",
    "dato = next(batch)\n",
    "dato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e064f97-f17d-4a2c-9e54-79a3b57fb407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecciona un ambiente CartPole\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# extrae tamaños de acciones y observaciones en el ambiente\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "# define tamaño capa oculta de la red\n",
    "HIDDEN_SIZE = 128\n",
    "#instancia un objeto Net\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "\n",
    "# define tamaño de los lotes\n",
    "BATCH_SIZE = 5\n",
    "# instancia un iterador Batch\n",
    "batch = Batch(env, net, BATCH_SIZE)\n",
    "\n",
    "# extrae el primer lote de datos\n",
    "dato = next(batch)\n",
    "dato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d234e-dabd-48d5-b462-9cc04a7c4084",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Clase Agent</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c3bc67-4b21-449a-bf40-0a22196e9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, batch_iterator, percentile, batch_size=1):\n",
    "        self.batch_iterator =  batch_iterator\n",
    "        self.percentile = percentile\n",
    "        self.batch_size = batch_size # batch size para los dataloaders\n",
    "        \n",
    "    def take_action(self): \n",
    "        # toma un lote de datos del iterador de lotes\n",
    "        batch = next(self.batch_iterator)\n",
    "        # extrae todas las recompensas del batch y hace una lista con ellas\n",
    "        rewards = list(map(lambda s: s.reward, batch))\n",
    "        # calcula la cota inferior para extraer los episodios élite (por defecto percentil 70)\n",
    "        reward_bound = np.percentile(rewards, self.percentile)\n",
    "        # calcula la recompensa promedio del lote de datos\n",
    "        reward_mean = float(np.mean(rewards))\n",
    "        \n",
    "        # extrae las observaciones y las respectivas acciones de los episodios élite\n",
    "        train_obs = []\n",
    "        train_act = []\n",
    "        for example in batch:\n",
    "            if example.reward < reward_bound:\n",
    "                continue\n",
    "            # de cada episodio élite extrae todas las parejas (observación, acción)\n",
    "            # agregando las observaciones en la lista de observaciones\n",
    "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
    "            # y las acciones en la lista de acciones\n",
    "            train_act.extend(map(lambda step: step.action, example.steps))\n",
    "            \n",
    "        # convierte listas a tensores\n",
    "        #train_obs = np.array(train_obs, dtype=np.float32)\n",
    "        train_obs_v = torch.FloatTensor(train_obs)\n",
    "        #train_act= np.array(train_obs, dtype=np.int64)\n",
    "        train_act_v = torch.LongTensor(train_act)\n",
    "        # crea el dataset\n",
    "        train_dataset = TensorDataset(train_obs_v, train_act_v)\n",
    "        # crea el dataloader                      \n",
    "        train_dataloader = DataLoader(dataset = train_dataset, batch_size = self.batch_size)\n",
    "        # entrega los datos\n",
    "        return train_dataloader, reward_bound, reward_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59800a93-8f57-4be3-b8ef-2bddac33eefb",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Prueba del agente (CartPole)</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e417050d-40fe-4d9f-bdd8-fc61d0615395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecciona un ambiente CartPole\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# extrae tamaños de acciones y observaciones en el ambiente\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "# define tamaño capa oculta de la red\n",
    "HIDDEN_SIZE = 128\n",
    "#instancia un objeto Net\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "\n",
    "# define tamaño de los lotes para el iterador de lotes\n",
    "BATCH_SIZE = 20\n",
    "# instancia un iterador Batch\n",
    "batch = Batch(env, net, BATCH_SIZE)\n",
    "\n",
    "# define el percentil para los episodios élite\n",
    "PERCENTILE = 70\n",
    "\n",
    "# Instancia un agente\n",
    "agent = Agent(batch, PERCENTILE)\n",
    "\n",
    "# entrega un conjunto de datos de los episodios élite de un  lote\n",
    "dato = agent.take_action()\n",
    "dato"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61429604-59e9-4ff6-af6a-2720a30c44dc",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Entrenamiento CartPole</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247e7077-64b5-4cc5-ab52-a9089a7634b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# selecciona un ambiente CartPole\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# extrae tamaños de acciones y observaciones en el ambiente\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "# define tamaño capa oculta de la red\n",
    "HIDDEN_SIZE = 128\n",
    "#instancia un objeto Net\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "\n",
    "# define tamaño de los lotes para el iterador de lotes\n",
    "BATCH_SIZE = 20\n",
    "# instancia un iterador Batch\n",
    "batch = Batch(env, net, BATCH_SIZE)\n",
    "\n",
    "# define el percentil para los episodios élite\n",
    "PERCENTILE = 70\n",
    "\n",
    "# Instancia un agente\n",
    "agent = Agent(batch, PERCENTILE)\n",
    "\n",
    "#instancia writer para tensorboard\n",
    "writer = SummaryWriter(comment=\"Entrenamiento de CartPole\")\n",
    "# agerga una grafo del modelo a tensorboard\n",
    "\n",
    "#episode = next(batch)\n",
    "#obs = episode.observation\n",
    "#writer.add_graph(net, obs)\n",
    "\n",
    "trainer = Trainer(model=net, writer=writer)\n",
    "\n",
    "# ciclo de entrenamiento\n",
    "min_reward = 200 # Para CartPole\n",
    "max_iterations = 300\n",
    "done = False\n",
    " \n",
    "iter_no = 0\n",
    "\n",
    "while not done:\n",
    "    iter_no += 1\n",
    "    # pide datos al agente\n",
    "    dataloader, reward_bound, reward_mean = agent.take_action()\n",
    "    # hace un paso de entrenamiento de la red\n",
    "      \n",
    "    trainer.fit(iter_no, dataloader, reward_bound, reward_mean)\n",
    "    #trainer.save_checkpoint()\n",
    "    #validation = trainer.validate(dataloaders=dataloader)\n",
    "    if reward_mean > min_reward:\n",
    "            print(\"Resuelto!\")\n",
    "            done = True\n",
    "    if iter_no == max_iterations:\n",
    "            print(\"Terminado por máximo número de iteraciones. No resuelto\")\n",
    "            done = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908297b-689f-4671-8e6d-4b57c7b82fd7",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Accediendo a tensorboard</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cb67ad-c4ca-4b2b-82cb-5afde1dca6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indica donde se escribirá el log de tensorboard\n",
    "!tensorboard --logdir './runs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2036ff7-f331-4a94-8251-59c2f37c4f94",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Entrenamiento FrozenLake</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35969654-1a8f-4b93-91fe-f2ba43760c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# selecciona un ambiente CartPole\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "\n",
    "# extrae tamaños de acciones y observaciones en el ambiente\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "# define tamaño capa oculta de la red\n",
    "HIDDEN_SIZE = 128\n",
    "#instancia un objeto Net\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "\n",
    "# define tamaño de los lotes para el iterador de lotes\n",
    "BATCH_SIZE = 100\n",
    "# instancia un iterador Batch\n",
    "batch = Batch(env, net, BATCH_SIZE)\n",
    "\n",
    "# define el percentil para los episodios élite\n",
    "PERCENTILE = 70\n",
    "\n",
    "# Instancia un agente\n",
    "agent = Agent(batch, PERCENTILE)\n",
    "\n",
    "trainer = Trainer(net)\n",
    "\n",
    "# ciclo de entrenamiento\n",
    "min_reward = 0.8 # Para FrozenLake\n",
    "max_iterations = 100\n",
    "done = False\n",
    " \n",
    "iter_no = 0\n",
    "\n",
    "while not done:\n",
    "    iter_no += 1\n",
    "    # pide datos al agente\n",
    "    dataloader, reward_bound, reward_mean = agent.take_action()\n",
    "    # hace un paso de entrenamiento de la red\n",
    "      \n",
    "    trainer.fit(iter_no, dataloader, reward_bound, reward_mean)\n",
    "    #trainer.save_checkpoint()\n",
    "    #validation = trainer.validate(dataloaders=dataloader)\n",
    "    if reward_mean > min_reward:\n",
    "            print(\"Resuelto!\")\n",
    "            done = True\n",
    "    if iter_no == max_iterations:\n",
    "            print(\"Terminado por máximo número de iteraciones. No resuelto\")\n",
    "            done = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a1092-abc0-45d4-abdc-dfcab95cccca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Modificación para el  ambiente FrozenLake</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36701e57-aa10-44bb-ba00-a87c49dd159b",
   "metadata": {},
   "source": [
    "Más adelante en este curso volveremos a este ambiente para resolver las limitaciones del MEC con otros métodos de aprendizaje reforzado.\n",
    "\n",
    "De momento haremos unas mejoras que ayuden al MEC a estimar la distribución de las acciones en el espacio de las observaciones. Haremos los siguiente:\n",
    "\n",
    "* Lotes de episodios más largos. Pasaremos a 100 episodios por lote.\n",
    "* Aplicaremos el factor de descuento $\\gamma$ a la recompensa. Así episodios más largos tendrán una menor recompensa y viceversa. Esto incrementa la variabilidad de la distribución de la recompensa.\n",
    "* Mantendremos episodios élite por más largo tiempo.\n",
    "* Decreceremos la rata de aprendizaje. Esto implica que la red neuronal tendrá más tiempo para ver en promedio más muestras de entrenamiento.\n",
    "* Mucho mayor tiempo de entrenamiento. \n",
    "\n",
    "Por favor revise, corra y modifique si lo considera necesario, el siguiente código.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e4d07b-a8cc-47da-af23-35dcd4ea7cd0",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Clase Agent2</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f6ea6-d903-4439-8d5e-adac83769468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "class Agent2(Agent):\n",
    "    def __init__(self, batch_iterator, percentile, batch_size=1, gamma=0.9):\n",
    "        super(Agent2, self).__init__(batch_iterator, percentile, batch_size)\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.full_batch = []\n",
    "        \n",
    "    def take_action(self): \n",
    "        # toma un lote de datos del iterador de lotes\n",
    "        batch = next(self.batch_iterator)\n",
    "        # agrega los datos que tenga preservados del episodio anterior\n",
    "        batch = batch + self.full_batch \n",
    "        # filtro para modificar la recompensa. Episodios mas largos tiene mayor descuento \n",
    "        filter_fun = lambda s: s.reward * (self.gamma** len(s.steps))\n",
    "        # extrae todas las recompensas del batch y hace una lista con ellas\n",
    "        disc_rewards = list(map(filter_fun, batch))\n",
    "        # calcula la cota inferior para extraer los episodios élite (por defecto percentil 70)\n",
    "        reward_bound = np.percentile(disc_rewards, self.percentile)\n",
    "        # calcula la recompensa promedio del lote de datos\n",
    "        reward_mean = float(np.mean(disc_rewards))\n",
    "        \n",
    "        # extrae las observaciones y las respectivas acciones de los episodios élite\n",
    "        train_obs = []\n",
    "        train_act = []\n",
    "        elite_batch = []\n",
    "        \n",
    "        for example, discounted_reward in zip(batch, disc_rewards):\n",
    "            if discounted_reward > reward_bound:\n",
    "                train_obs.extend(map(lambda step: step.observation,\n",
    "                                     example.steps))\n",
    "                train_act.extend(map(lambda step: step.action,\n",
    "                                     example.steps))\n",
    "                elite_batch.append(example)\n",
    "        # guarda este batch élite para el siguiente episodio\n",
    "        self.full_batch = elite_batch[-500:] # conserva los últimos 500 datos\n",
    "            \n",
    "        # convierte listas a tensores\n",
    "        train_obs_v = torch.FloatTensor(train_obs)\n",
    "        train_act_v = torch.LongTensor(train_act)\n",
    "        # crea el dataset\n",
    "        train_dataset = TensorDataset(train_obs_v, train_act_v)\n",
    "        # crea el dataloader                      \n",
    "        train_dataloader = DataLoader(dataset = train_dataset, batch_size = self.batch_size)\n",
    "        # entrega los datos\n",
    "        return train_dataloader, reward_bound, reward_mean\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06be220-d9a6-4ceb-9fdf-523110f7ab4c",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Re-entrenamiento FrozenLake</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786992fa-9c12-4114-b0a9-577e2cb3a235",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db086dc3-1e0e-4cf0-9f53-04df20139910",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# indica donde se escribirá el log de tensorboard\n",
    "tensorboard --logdir=runs\n",
    "\n",
    "# selecciona un ambiente CartPole\n",
    "env = DiscreteOneHotWrapper(gym.make(\"FrozenLake-v1\"))\n",
    "\n",
    "# extrae tamaños de acciones y observaciones en el ambiente\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "# define tamaño capa oculta de la red\n",
    "HIDDEN_SIZE = 128\n",
    "#instancia un objeto Net\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "\n",
    "# define tamaño de los lotes para el iterador de lotes\n",
    "BATCH_SIZE = 100\n",
    "# instancia un iterador Batch\n",
    "batch = Batch(env, net, BATCH_SIZE)\n",
    "\n",
    "# define el percentil para los episodios élite\n",
    "PERCENTILE = 50\n",
    "\n",
    "# Instancia un agente\n",
    "agent = Agent2(batch, PERCENTILE)\n",
    "\n",
    "# instancia writer\n",
    "writer = SummaryWriter(comment=\"Entrenamiento de CartPole\")\n",
    "\n",
    "trainer = Trainer(model=net, writer=writer)\n",
    "\n",
    "# ciclo de entrenamiento\n",
    "min_reward = 0.8 # Para FrozenLake\n",
    "max_iterations = 1000\n",
    "done = False\n",
    " \n",
    "iter_no = 0\n",
    "\n",
    "while not done:\n",
    "    iter_no += 1\n",
    "    # pide datos al agente\n",
    "    dataloader, reward_bound, reward_mean = agent.take_action()\n",
    "    # hace un paso de entrenamiento de la red\n",
    "      \n",
    "    trainer.fit(iter_no, dataloader, reward_bound, reward_mean)\n",
    "    #trainer.save_checkpoint()\n",
    "    #validation = trainer.validate(dataloaders=dataloader)\n",
    "    if reward_mean > min_reward:\n",
    "            print(\"Resuelto!\")\n",
    "            done = True\n",
    "    if iter_no == max_iterations:\n",
    "            print(\"Terminado por máximo número de iteraciones. No resuelto\")\n",
    "            done = True\n",
    "\n",
    "# envia al write cualquier cálculo pendiente\n",
    "writer.flush()\n",
    "# cierra el writer\n",
    "close(writer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7f62",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ac7d2",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)\n",
    "1. [Solving a Reinforcement Learning Problem Using Cross-Entropy Method](https://towardsdatascience.com/solving-a-reinforcement-learning-problem-using-cross-entropy-method-23d9726a737)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
