{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Gradiente de la política)=\n",
    "# <span style=\"color:#F72585\"><center>Gradientes de la Política</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> Policy gradients</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Archivos_Generales/Imagenes/Carretera.jpg\" width=\"600\" height=\"400\" align=\"center\" /> \n",
    "</center>   \n",
    "</figure>\n",
    "\n",
    "<center>Fuente: <a href=\"https://sp.depositphotos.com/stock-photos/architecture-buildings.html\">Dubai downtown cityscape - sp.depositphotos.com</a></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las lecciones previas hemos hablado de la política de un agente, pero no hemos desarrollado ningún procedimiento que directamente la incluya como parte central de los algoritmos.\n",
    "\n",
    "Por otro lado, hasta el momento hemos trabajado con modelos que tienen pocas acciones posibles, por lo que el método de entropía cruzada y el método Q-learning y el método DQN, los cuales están basados en redes neuronales de clasificación aplican bien al tipo de problemas abordado. Pero, ¿Qué sucede si el número de posibles acciones es muy grande o incluso infinito o no contable?\n",
    "\n",
    "Piense por un momento en el problema de conducción automática. Una acción posible es girar el timón para cambiar ligeramente la dirección para por ejemplo evitar un obstáculo. El ángulo de giro que corresponde a la respectiva acción es un número, posiblemente un número real.\n",
    "\n",
    "\n",
    "De otro lado, nos hemos enfocado en los valores de los estados o de las acciones dados los estados (Q-valores). Estos son números que, aunque están vinculados con la política, no son probabilidades. \n",
    "\n",
    "En la práctica, cambios ligeros en el proceso de optimización de las funciones de pérdida pueden ocasionar cambios importantes en estos valores. Por otro lado, si la salida de la red neuronal es un vector de probabilidades, tales cambios por lo general resultan más suaves. Por todas razones, en esta lección pondremos nuestro objetivo en la política del agente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">La hipótesis de la recompensa</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El aprendizaje reforzado (AR) se basa en que toda las metas y propósitos de un agente pueden ser pensados en términos de la optimización del valor esperado de la suma acumulativa de una señal escalar recibida que hemos llamado recompensa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Procesos de decisión de Markov</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El agente trabaja siguiendo un proceso de decisión de Markov (PDM). el cual consiste de una decisión (¿Cuál acción tomar?) que debe tomar en cada estado. Esto da origen a una sucesión de estados, acciones y recompensas llamada  `trayectoria` y que se puede visualizar como\n",
    "\n",
    "$$\n",
    "s_0, a_0, r_1, s_1, a_1,r_2,\\cdots\n",
    "$$\n",
    "\n",
    "y el objetivo es maximizar este conjunto de recompensas.\n",
    "\n",
    "Un `Proceso de decisión de Markov (PDM)` es una tupla $(S,A,R,p, \\gamma)$ tal que\n",
    "\n",
    "$$\n",
    "p(s',r| s,a) = Pr[s_{t+1}= s', R_{t+1}=r|S_t=s, A_t=a]\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "en donde $S_t \\in S$ (espacio de estados), $A_t \\in A$(espacio de acciones), $R_t \\in R$ (espacio de recompensas) y $p$ se denomina dinámica del ambiente. \n",
    "\n",
    "En palabras simples un PDM define la probabilidad de transición a un nuevo estado $s'$ recibir una recompensa $r$ partiendo del estado actual $s$, y si se ejecuta la acción $a$.\n",
    "\n",
    "Técnicamente es un proceso de Markov de primer orden. Un elemento importante del modelo es el factor de descuento $\\gamma$, el cual es un valor entre 0 y 1. Sumando las recompensas futuras a lo largo del tiempo, descontadas con una potencia del factor de descuento se obtiene el concepto de retorno $G_t$. Técnicamente hemos definido el retorno en el tiempo $t$ como\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2R_{t+3} + \\cdots\n",
    "$$\n",
    "\n",
    "\n",
    "La idea central en AR es que el agente encuentre trayectorias que maximizan el valor esperado del retorno.\n",
    "\n",
    "La dinámica del ambiente $p$ está fuera del alcance del agente. Recuerde el juego de Frozen Lake. Un ejemplo de la vida real puede ser el siguiente. Suponga que se encuentra en un lugar con demasiado viento y conduce un vehículo muy liviano. O puede imaginarse en un velero. Se puede intentar ir en una determinada dirección (la acción). Pero el viento extremo lo puede conducir en otra dirección. Sin embargo, puede ser que sea posible elegir una dirección diferente que permita ir en la dirección correcta. Esta es la política, que el agente si controla.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Política</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando un agente sigue una política $\\pi$, genera una sucesión de estados, acciones y recompensas que denominaremos trayectoria. Técnicamente la política se define como la probabilidad de acciones dado un estado:\n",
    "\n",
    "$$\n",
    "\\pi(A_t=a|S_t=s), \\hspace{3mm} \\forall A_t\\in A(s), S_t\\in S.\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Gradientes de política</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "El `objetivo del aprendizaje reforzado (AR) es maximizar la recompensa $r$ cuando sigue un política parametrizada` $\\pi_{\\theta}$. Si $r(\\tau)$ representa la recompensa total para una trayectoria $\\tau$, denotaremos como función objetivo (target) a la función  $J(\\theta)$ definida por\n",
    "\n",
    "$$\n",
    "J(\\theta)= \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)],\n",
    "$$\n",
    "\n",
    "es decir, nos interesa encontrar la política $\\pi_{\\theta}$ que maximiza la recompensa esperada. Observe que\n",
    "\n",
    "$$\n",
    "J(\\theta)= \\int r(\\tau) \\pi_{\\theta}(\\tau)d\\tau.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Puede demostrarse que bajos ciertos supuestos que generalmente se tienen en AR `todo PDM finito tiene al menos una política optima en el sentido de la recompensa obtenida y que entre todas las políticas optimales al menos una es estacionaria y determinista`.\n",
    "\n",
    "El procedimiento clásico para estimar el parámetro $\\theta$ es el método del gradiente descendiente, el cual (en términos muy simples) se basa en la regla de actualización\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t + \\alpha \\nabla J(\\theta_t).\n",
    "$$\n",
    "\n",
    "$\\nabla J(\\theta_t)$ es el gradiente de la función objetivo y $\\alpha$ una rata de aprendizaje.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "El reto que tenemos es encontrar el gradiente de la política. El primer  problema es que $J(\\theta)$ está definido como una esperanza (¡una integral!). \n",
    "\n",
    "Bajo algunas condiciones relacionadas con las derivadas de $\\pi$  es posible pasar el gradiente a través de la integral como sigue\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] &= \\nabla \\int \\pi_{\\theta}(\\tau)r(\\tau)d\\tau\\\\\n",
    "& =   \\int\\nabla \\pi_{\\theta}(\\tau)r(\\tau)d\\tau\\\\\n",
    "& =   \\int \\pi_{\\theta}(\\tau)\\nabla \\log\\pi_{\\theta}(\\tau)r(\\tau)d\\tau\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Por lo que se tiene el siguiente resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Teorema del gradiente de la política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "$$\n",
    "\\nabla J(\\theta_t) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)\\nabla \\log \\pi_{\\theta}(\\tau)]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se expande la definición de $\\pi_{\\theta}(\\tau)$ se obtiene para una trayectoria de $T$ pasos\n",
    "\n",
    "$$\n",
    "\\pi_{\\theta}(\\tau) = P(s_0)\\prod_{t=1}^{T}\\pi_{\\theta}(a_t|s_t)p(s_{t+1}, r_{t+1}|s_t, a_t).\n",
    "$$\n",
    "\n",
    "$P$ representa la distribución del estado inicial $s_0$ y se ha aplicado la regla del producto de la probabilidad y el hecho de ser un proceso de Markov que implica que cada nueva acción es independiente de la anterior. $T$ representa la longitud de la trayectoria. \n",
    "\n",
    "Tomando logaritmo se obtiene que\n",
    "\n",
    "$$\n",
    "\\log \\pi_{\\theta}(\\tau) = \\log P(s_0) + \\sum_{t=1}^{T}\\log \\pi_{\\theta}(a_t|s_t) + \\sum_{t=1}^{T} \\log p(s_{t+1}, r_{t+1}|s_t, a_t).\n",
    "$$\n",
    "\n",
    "Por lo tanto se tiene que\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] = \\mathbb{E}_{\\pi_{\\theta}}\\left[ r(\\tau) \\left(\\sum_{t=1}^{T}\\nabla \\log \\pi_{\\theta}(a_t|s_t)\\right) \\right]\n",
    "$$\n",
    "\n",
    "Este resultado dice que no es necesario conocer la distribución del estado inicial $P$ ni la dinámica del ambiente $p$ para calcular el gradiente de la política.\n",
    "\n",
    "Los algoritmos que usan este resultado son conocidos como `algoritmos libres del modelo`, debido a que no modelamos el ambiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">MCMC</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la última ecuación se tiene que el cálculo del gradiente involucra una esperanza, es decir una integral, la cual por lo general es intratable. Aquí es en donde entran en acción las técnicas Monte Carlo Markov Chain (MCMC). La idea central es que en cada paso de tiempo (cada iteración) se hace lo siguiente:\n",
    "\n",
    "1. Se obtiene una muestra aleatoria grande de acciones siguiendo la distribución (política) $\\pi(\\theta^*)$, en donde $\\theta^*$ es la estimación actual del parámetro $\\theta$.\n",
    "1. Se calcula el promedio de la expresión del gradiente para la muestra obtenida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Recompensa de la trayectoria</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El termino $r(\\tau)$ de la recompensa total de la trayectoria $\\tau$ permanece inmutable en la expresión de $\\nabla J(\\theta)$. Este gradiente parametrizado no depende de  $r(\\tau)$. Sin embargo, el termino agrega bastante varianza en el muestreo MCMC. En realidad, hay $T$ fuentes de variación debidas a cada $R_t$. En su lugar podemos hacer uso del retorno $G_t$ debido a que desde el punto de vista de la optimización del objetivo del AR, la recompensa pasada no contribuye para nada.\n",
    "\n",
    "Cuando se reemplaza $r(\\tau)$ por el retorno descontado $G_t$ arribamos al algoritmo clásico del gradiente de la política llamado reforzamiento (`reinforce`)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Método Refuerzo (Reinforce) </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El algoritmo reinforce se basa en aproximar el gradiente de la política como \n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] \\approx \\mathbb{E}_{\\pi_{\\theta}}\\left[  \\sum_{t=1}^{T}G_t \\nabla\\log \\pi_{\\theta}(a_t|s_t) \\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En realidad, en la formula anterior no se ha resuelto el problema de la varianza en las trayectorias muestreadas. Desde el punto de vista de la inferencia bayesiana, se sabe que cuando el tamaño de muestra es grande, no importa la distribución a priori seleccionada. Esto implica que con muestras grandes de trayectorias la distribución del estado inicial no es importante y el algoritmo MCMC converge al modelo de los parámetros verdaderos. El problema es que con varianzas muy grandes lograr estabilizar los parámetros del modelo es bastante difícil. Resolveremos este problema en la lección {ref}`Actor-crítico`.\n",
    "\n",
    "De momento examinemos la implementación usual del método `reinforce`. Como hemos hecho antes a lo largo del curso, vamos a usar la aproximación del retorno con la función de Q-valores $Q(s,a)$. Así, para cada paso de la trayectoria tenemos que\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) \\approx \\mathbb{E}_{\\pi_{\\theta}}\\left[  Q(s,a) \\nabla\\log \\pi_{\\theta}(a|s) \\right]\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Interpretación</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El gradiente de la política indica la dirección en la cual se deben cambiar los parámetros de la red neuronal para mejorar la política en términos de la recompensa total acumulada. En esta aproximación el gradiente se escala proporcionalmente al valor de la acción tomada $Q(s,a)$ y el gradiente en sí mismo es igual al gradiente del logaritmo de la probabilidad de la acción tomada. \n",
    "\n",
    "Esto significa que estamos tratando de aumentar la probabilidad de acciones que nos han dado buena recompensa total y disminuyen la probabilidad de acciones con\n",
    "malos resultados finales. \n",
    "\n",
    "La esperanza es aproximada mediante el promedio del gradiente en varios pasos, de acuerdo con las técnicas MCMC.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Función de pérdida y máxima log-verosimilitud</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de pérdida que usualmente se utiliza en los métodos de gradiente de la política es\n",
    "\n",
    "$$\n",
    "\\mathfrak{L}(\\theta) = -Q(s,a) \\log\\pi_{\\theta}(a|s)\n",
    "$$\n",
    "\n",
    "Es función de pérdida es menos la log-verosimilitud del modelo estadístico\n",
    "\n",
    "$$\n",
    "L \\propto \\pi_{\\theta}(a|s)^{Q(s,a)}.\n",
    "$$\n",
    "\n",
    "Por ejemplo, para una trayectoria $\\tau$ con $T$ pasos se tiene que\n",
    "\n",
    "$$\n",
    "\\log L(\\tau) = \\sum_{t=1}^{T} Q(s_t,a_t)\\log\\pi_{\\theta}(a_t|s_t)\n",
    "$$\n",
    "\n",
    "Finalmente, observe que\n",
    "\n",
    "$$\n",
    "\\nabla \\mathfrak{L}(\\theta) = -Q(s,a) \\nabla \\log\\pi_{\\theta}(a|s)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">El algortimo Reinforce</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Inicialice la red neuronal con pesos aleatorios.\n",
    "1. Corra N episodios completos (trayectorias completas), almacenando las transiciones $(s,a,r,s')$.\n",
    "1. Para cada paso, $t$, de cada uno de los episodios,  $k$, calcule la recompensa total descontada para la sucesión de pasos: $Q_{t,k}= \\sum_{i=0} \\gamma^i r_i$.\n",
    "1. Calcule la función de pérdida para todas las transiciones: $\\mathfrak{L} = - \\sum_{tk} Q_{t,k} \\log(\\pi_{\\theta}(s_{t,k},a_{t,k}))$\n",
    "1. Ejecute el paso de actualización de pesos del algoritmo SGD seleccionado.\n",
    "1. Repita desde el paso 2 hasta convergencia.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Diferencias con Q-learning</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método reinforce se diferencia del método Q-learning en varias cosas.\n",
    "\n",
    "1. No se requiere una exploración explícita de tipo epsilon-greedy, que se usa en Q-learning para evitar que el algoritmo quede estancado en un mínimo local. Ahora la red neuronal retorna probabilidades directamente. Al comienzo la red es inicializada con valores aleatorios, por lo que la primera salida corresponde a una distribución uniforme.\n",
    "1. No se usa la memoria de repetición (replay buffer). Los métodos gradiente de política son métodos basados en la política (Q-learning es libre de política). Es decir, los datos obtenidos para el entrenamiento no se basan en políticas viejas. Esto es bueno y malo. Lo bueno es que estos métodos convergen más rápido por lo general. Lo malo es que requieren por lo general mucha más interacción con el ambiente que por ejemplo DQN.\n",
    "1. No se requiere una red neuronal objetivo (target). Aquí se usan los Q-valores, pero ya no son aproximados como DQN, sino que se calculan completamente en cada trayectoria. En DQN se requiere la red target para romper la correlación entre valores $Q(s,a)$, porque en cada paso ellos son aproximados. Aquí los Q-valores se calculan sin aproximación.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\"> Ejemplo CartPole </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección vemos el método en acción. usaremos el problema CartPole para ilustrar el método con código real.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Importa  módulos</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "#import ptan\n",
    "import numpy as np\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "#ptan\n",
    "import actions\n",
    "import agent\n",
    "import experience\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Hiperparámetros generales</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.01\n",
    "EPISODES_TO_TRAIN = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Clase PGN (Policy Gradient Network)</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta es la red neuronal que usaremos. Ya familiar para todos.\n",
    "Observe que la red no regresa probabilidades como hemos previsto. Esto se hace porque en la implementación decidimos usar en la perdida la función *log_softmax* que calcula de manera eficiente el logaritmo del softmax que en realidad es lo que se requiere para la función de pérdida. Además, este cálculo es bastante más estable que calcular inicialmente softmax y luego el logaritmo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Cálculo de los Q-valores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con la siguiente función se hace el cálculo de los Q-valores de manera eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_qvals(rewards):\n",
    "    res = []\n",
    "    sum_r = 0.0\n",
    "    for r in reversed(rewards):\n",
    "        sum_r *= GAMMA\n",
    "        sum_r += r\n",
    "        res.append(sum_r)\n",
    "    return list(reversed(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Entrenamiento</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGN(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\envs\\registration.py:555: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'agent' has no attribute 'PolicyAgent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\User\\OneDrive\\Documentos\\GitHub\\Libro-Fundamentos\\Aprendizaje_Reforzado\\Cuadernos\\ar_Gradientes_Politica.ipynb Celda 48\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documentos/GitHub/Libro-Fundamentos/Aprendizaje_Reforzado/Cuadernos/ar_Gradientes_Politica.ipynb#Y110sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m net \u001b[39m=\u001b[39m PGN(env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mn)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documentos/GitHub/Libro-Fundamentos/Aprendizaje_Reforzado/Cuadernos/ar_Gradientes_Politica.ipynb#Y110sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(net)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documentos/GitHub/Libro-Fundamentos/Aprendizaje_Reforzado/Cuadernos/ar_Gradientes_Politica.ipynb#Y110sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m agente \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mPolicyAgent(net, preprocessor\u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mfloat32_preprocessor,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documentos/GitHub/Libro-Fundamentos/Aprendizaje_Reforzado/Cuadernos/ar_Gradientes_Politica.ipynb#Y110sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                                apply_softmax\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documentos/GitHub/Libro-Fundamentos/Aprendizaje_Reforzado/Cuadernos/ar_Gradientes_Politica.ipynb#Y110sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m exp_source \u001b[39m=\u001b[39m experience\u001b[39m.\u001b[39mExperienceSourceFirstLast(env, agente, gamma\u001b[39m=\u001b[39mGAMMA) \u001b[39m# iterator\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/User/OneDrive/Documentos/GitHub/Libro-Fundamentos/Aprendizaje_Reforzado/Cuadernos/ar_Gradientes_Politica.ipynb#Y110sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(net\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mLEARNING_RATE)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'agent' has no attribute 'PolicyAgent'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    #writer = SummaryWriter(comment=\"-cartpole-reinforce\")\n",
    "\n",
    "    net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "\n",
    "    agente = agent.PolicyAgent(net, preprocessor= agent.float32_preprocessor,\n",
    "                                   apply_softmax=True)\n",
    "    exp_source = experience.ExperienceSourceFirstLast(env, agente, gamma=GAMMA) # iterator\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "\n",
    "    batch_episodes = 0\n",
    "    batch_states, batch_actions, batch_qvals = [], [], []\n",
    "    cur_rewards = []\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(int(exp.action))\n",
    "        cur_rewards.append(exp.reward)\n",
    "\n",
    "        if exp.last_state is None:\n",
    "            batch_qvals.extend(calc_qvals(cur_rewards))\n",
    "            cur_rewards.clear()\n",
    "            batch_episodes += 1\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, done_episodes))\n",
    "            #writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            #writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            #writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 195:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "\n",
    "        if batch_episodes < EPISODES_TO_TRAIN:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_qvals_v = torch.FloatTensor(batch_qvals)\n",
    "\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = batch_qvals_v * log_prob_v[range(len(batch_states)), batch_actions_t]\n",
    "        loss_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_episodes = 0\n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_qvals.clear()\n",
    "\n",
    "    #writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">Comparación con DQN</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente imagen tomada del texto del Maxi Lapan, página 292, se compara en condiciones similares el comportamiento de los algoritmos DQN y REINFORCE. Los datos por supuesto dependen de los valores pseudo aleatorios iniciales, pero claramente el problema es resuelto mucho más rápido con REINFORCE. Esto se conoce en la literatura y ha sido  la motivación de estudiar el método. En la siguiente sección hacemos una ampliación en la cual se trata de resolver el problema de la varianza introducida en el gradiente de política por el retorno $G_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Aprendizaje_Reforzado/Imagenes/DQN_Reinforce.png\"  width=\"500\" height=\"400\" align=\"center\"/> \n",
    "</figure>\n",
    "\n",
    "Fuente: Maxi Lapan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Métodos de política v.s. Métodos basados en valores</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los métodos de política son una generalización del método de entropía cruzada.\n",
    "\n",
    "+ Los métodos de política optimizan directamente la política. Los  métodos de valor, como DQN, hacen lo mismo indirectamente.\n",
    "+ Los métodos de política están basados totalmente en la política y requieren muestras frescas del ambiente. Los métodos de valor pueden beneficiarse de datos antiguos, obtenidos de la vieja política y otras fuentes.\n",
    "+ Los métodos de política suelen ser menos eficientes en la muestra, lo que significa que requieren mayor interacción con el entorno. Los métodos de valor pueden beneficiarse de grandes búferes de reproducción (replay buffer). Sin embargo, la eficiencia de la muestra no significa que los métodos de valor son más eficientes desde el punto de vista computacional y, muy a menudo, es todo lo contrario.\n",
    "+ En el ejemplo anterior, durante el entrenamiento, necesitábamos acceder a nuestro RN una sola vez, para obtener las probabilidades de las acciones. En DQN, necesitamos procesar dos lotes de estados: uno para el estado actual y otro para el estado siguiente en la actualización de Bellman.\n",
    "\n",
    "En general hay situaciones en donde DQN es más natural. Por ejemplo, las soluciones del estado del arte para juegos de Atari son variaciones de DQN. En contraste los métodos de política son mejor aplicados en problemas de control continuo o en casos en donde el acceso al ambiente de barato y rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Refuerzo con línea base </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ya mencionamos antes que nuestra aproximación del gradiente de política dada por $\\nabla J \\approx \\mathbb{E}[Q(s,a)\\nabla \\pi(s|a)]$ es proporcional a la recompensa descontada desde un estado dado. Esta recompensa depende por supuesto del ambiente e introduce una fuente importante de variación. Una solución a este inconveniente substraer de la recompensa una contante. Posibles soluciones son restar de la recompensa\n",
    " \n",
    "+ La media de las recompensas descontadas.\n",
    "+ La media móvil de las recompensas descontadas.\n",
    "+ El valor del estado V(s).\n",
    "\n",
    "La última solución es introducida en la lección actor-crítico.\n",
    "\n",
    "Por otro lado, para recortar los episodios y no hacer cálculos innecesarios se puede determinar a partir de qué momento el factor de descuento es tan pequeño que más pasos en el episodio no aportan mucho al cálculo de la recompensa descontada. Por ejemplo, $0.9^{50}= 0.005$. Entonces si el factor de descuento es $\\gamma=0.9$, quizás sea suficiente parar después de 50 pasos en un episodio.\n",
    "\n",
    "\n",
    "La introducción de un constante que reste a la recompensa descontada se basa en el siguiente hecho.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede verificarse que\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[\\left( \\nabla \\log \\pi_{\\theta}(a_t|s_t) \\right) \\right]=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En efecto, bajo el supuesto que la integral y el gradiente pueden intercambiarse, se tiene que\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[\\left( \\nabla \\log \\pi_{\\theta}(a|s) \\right) \\right] =\\int \\pi_{\\theta}(a|s)\\nabla \\log \\pi_{\\theta}(a|s)d\\tau =  \\int \\nabla \\pi_{\\theta}(a|s)d\\tau= \\nabla\\int  \\pi_{\\theta}(a|s)d\\tau = \\nabla  1 = 0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La verificación meticulosa, se  deja como ejercicio al lector interesado. Sea $b$ una variable que no depende de la trayectoria $\\tau$.  Esta variable se denominará `línea base`. Debido al resultado anterior se tiene que\n",
    "\n",
    "$$\n",
    "\\nabla J(\\theta) = \\nabla \\mathbb{E}_{\\pi_{\\theta}}[r(\\tau)] \\approx \\mathbb{E}_{\\pi_{\\theta}}\\left[  \\sum_{t=1}^{T}(G_t-b)\\nabla\\log \\pi_{\\theta}(a_t|s_t) \\right].\n",
    "$$\n",
    "\n",
    "Usando $b$ en teoría y en la práctica la varianza puede ser reducida, manteniendo el gradiente de la política insesgado. Un buen valor que puede usarse como línea base el valor del estado actual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejemplo de implementación de Gradiente de política con línea base</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import gym\n",
    "import ptan\n",
    "import numpy as np\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 0.001\n",
    "ENTROPY_BETA = 0.01\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "REWARD_STEPS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PGN(nn.Module):\n",
    "    def __init__(self, input_size, n_actions):\n",
    "        super(PGN, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(old: Optional[float], val: float, alpha: float = 0.95) -> float:\n",
    "    if old is None:\n",
    "        return val\n",
    "    return old * alpha + (1-alpha)*val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    #writer = SummaryWriter(comment=\"-cartpole-pg\")\n",
    "\n",
    "    net = PGN(env.observation_space.shape[0], env.action_space.n)\n",
    "    print(net)\n",
    "\n",
    "    agent = ptan.agent.PolicyAgent(net, preprocessor=ptan.agent.float32_preprocessor,\n",
    "                                   apply_softmax=True)\n",
    "    exp_source = ptan.experience.ExperienceSourceFirstLast(\n",
    "        env, agent, gamma=GAMMA, steps_count=REWARD_STEPS)\n",
    "\n",
    "    optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    total_rewards = []\n",
    "    step_rewards = []\n",
    "    step_idx = 0\n",
    "    done_episodes = 0\n",
    "    reward_sum = 0.0\n",
    "    bs_smoothed = entropy = l_entropy = l_policy = l_total = None\n",
    "\n",
    "    batch_states, batch_actions, batch_scales = [], [], []\n",
    "\n",
    "    for step_idx, exp in enumerate(exp_source):\n",
    "        reward_sum += exp.reward\n",
    "        baseline = reward_sum / (step_idx + 1)\n",
    "        #writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "        batch_states.append(exp.state)\n",
    "        batch_actions.append(int(exp.action))\n",
    "        batch_scales.append(exp.reward - baseline)\n",
    "\n",
    "        # handle new rewards\n",
    "        new_rewards = exp_source.pop_total_rewards()\n",
    "        if new_rewards:\n",
    "            done_episodes += 1\n",
    "            reward = new_rewards[0]\n",
    "            total_rewards.append(reward)\n",
    "            mean_rewards = float(np.mean(total_rewards[-100:]))\n",
    "            print(\"%d: reward: %6.2f, mean_100: %6.2f, episodes: %d\" % (\n",
    "                step_idx, reward, mean_rewards, done_episodes))\n",
    "            #writer.add_scalar(\"reward\", reward, step_idx)\n",
    "            #writer.add_scalar(\"reward_100\", mean_rewards, step_idx)\n",
    "            #writer.add_scalar(\"episodes\", done_episodes, step_idx)\n",
    "            if mean_rewards > 195:\n",
    "                print(\"Solved in %d steps and %d episodes!\" % (step_idx, done_episodes))\n",
    "                break\n",
    "\n",
    "        if len(batch_states) < BATCH_SIZE:\n",
    "            continue\n",
    "\n",
    "        states_v = torch.FloatTensor(batch_states)\n",
    "        batch_actions_t = torch.LongTensor(batch_actions)\n",
    "        batch_scale_v = torch.FloatTensor(batch_scales)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits_v = net(states_v)\n",
    "        log_prob_v = F.log_softmax(logits_v, dim=1)\n",
    "        log_prob_actions_v = batch_scale_v * log_prob_v[range(BATCH_SIZE), batch_actions_t]\n",
    "        loss_policy_v = -log_prob_actions_v.mean()\n",
    "\n",
    "        prob_v = F.softmax(logits_v, dim=1)\n",
    "        entropy_v = -(prob_v * log_prob_v).sum(dim=1).mean()\n",
    "        entropy_loss_v = -ENTROPY_BETA * entropy_v\n",
    "        loss_v = loss_policy_v + entropy_loss_v\n",
    "\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # calc KL-div\n",
    "        new_logits_v = net(states_v)\n",
    "        new_prob_v = F.softmax(new_logits_v, dim=1)\n",
    "        kl_div_v = -((new_prob_v / prob_v).log() * prob_v).sum(dim=1).mean()\n",
    "        #writer.add_scalar(\"kl\", kl_div_v.item(), step_idx)\n",
    "\n",
    "        grad_max = 0.0\n",
    "        grad_means = 0.0\n",
    "        grad_count = 0\n",
    "        for p in net.parameters():\n",
    "            grad_max = max(grad_max, p.grad.abs().max().item())\n",
    "            grad_means += (p.grad ** 2).mean().sqrt().item()\n",
    "            grad_count += 1\n",
    "\n",
    "        bs_smoothed = smooth(bs_smoothed, np.mean(batch_scales))\n",
    "        entropy = smooth(entropy, entropy_v.item())\n",
    "        l_entropy = smooth(l_entropy, entropy_loss_v.item())\n",
    "        l_policy = smooth(l_policy, loss_policy_v.item())\n",
    "        l_total = smooth(l_total, loss_v.item())\n",
    "        \"\"\"\n",
    "        writer.add_scalar(\"baseline\", baseline, step_idx)\n",
    "        writer.add_scalar(\"entropy\", entropy, step_idx)\n",
    "        writer.add_scalar(\"loss_entropy\", l_entropy, step_idx)\n",
    "        writer.add_scalar(\"loss_policy\", l_policy, step_idx)\n",
    "        writer.add_scalar(\"loss_total\", l_total, step_idx)\n",
    "        writer.add_scalar(\"grad_l2\", grad_means / grad_count, step_idx)\n",
    "        writer.add_scalar(\"grad_max\", grad_max, step_idx)\n",
    "        writer.add_scalar(\"batch_scales\", bs_smoothed, step_idx)\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_states.clear()\n",
    "        batch_actions.clear()\n",
    "        batch_scales.clear()\n",
    "\n",
    "    #writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Adaptado de Rowel Atienza, Advance Deep Learning with Tensorflow 2 and Keras,Pack,2020](https://www.amazon.com/-/es/Rowel-Atienza-ebook/dp/B0851D5YQQ).\n",
    "1. [Sutton, R. S., & Barto, A. G. (2018).Reinforcement learning: An introductio, MIT Press, 2018](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)\n",
    "1. [Ejecutar en Colab](https://colab.research.google.com/drive/1ExE__T9e2dMDKbxrJfgp8jP0So8umC-A#sandboxMode=true&scrollTo=2XelFhSJGWGX)\n",
    "1. [Human-level control through deep reinforcement\n",
    "learning](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of part1-MultiarmedBandit.ipynb",
   "provenance": [
    {
     "file_id": "1oqn00G-A4s_c8n6FoVygfQjyWl6BKy_u",
     "timestamp": 1603810835075
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3.9.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf92aa13fedf815d5c8dd192b8d835913fde3e8bc926b2a0ad6cc74ef2ba3ca2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
