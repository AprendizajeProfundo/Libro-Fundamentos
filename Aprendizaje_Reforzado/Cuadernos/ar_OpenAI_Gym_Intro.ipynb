{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b718a9ce",
   "metadata": {},
   "source": [
    "# <span style=\"color:#F72585\"><center>Introducción a OpenAI Gym</center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383b39b-7af1-4d0d-8dcc-97809d8dd444",
   "metadata": {
    "tags": []
   },
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Aprendizaje_Reforzado/Imagenes/Hide_and_seek.jpg\" width=\"700\" height=\"500\" align=\"center\"/>\n",
    "</center> \n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [Hide and seek. Wiki commons](https://commons.wikimedia.org/w/index.php?search=hide+and+seek&title=Special:MediaSearch&go=Go&type=image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be027523",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e665ee-642b-485c-ab28-599707769be4",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Aprendizaje_Reforzado/Imagenes/multi-agent-policy-architecture.png\" width=\"500\" height=\"400\" align=\"center\"/>\n",
    "</center> \n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: [OpenAI-blog](https://openai.com/blog/emergent-tool-use/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cc08e0",
   "metadata": {},
   "source": [
    "En esta lección se presentan los elementos esenciales de la API de OpenAI Gym. Este ha sido un esfuerzo de OpenAI para poner a nuestra disposición una gran cantidad de ambientes, de muy diversa índole que incluye problema clásicos de control optimal, como el que estudiamos hoy, juegos clásicos y más.\n",
    "\n",
    "La API es bastante simple y esta desarrollada para que muy rápidamente sea posible interactuar con ella para permitirnos desarrollar nuestros propios modelos de aprendizaje reforzado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b04fc1-8074-40f9-9db6-48d94a3ec192",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">La API OpenAI Gym</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b47fc87-8d1f-47c6-99e4-0ec41164d55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instalar gym desde conda\n",
    "#conda install -c conda-forge gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546c7e44-8ec1-4053-b7af-4a3df4dbf81d",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Aprendizaje_Reforzado/Imagenes/agente-ambiente.png\" width=\"600\" height=\"500\" align=\"center\"/>\n",
    "</center> \n",
    "</figure>\n",
    "\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c860ae9-f443-4871-b813-3c23750fb9c6",
   "metadata": {},
   "source": [
    "La API `OpenAI Gym` tiene una rica colección de ambientes para experimentos de aprendizaje reforzado (AR), usando una interfaz unificada.\n",
    "\n",
    "La clase principal de Gym es `Env` (de environment). Sus métodos y atributos proveen la información necesaria para poder entrenar agentes que interactúen con el medio ambientes. Las piezas más importante disponibles con *Env* son:\n",
    "\n",
    "- Un conjunto de `acciones` que se permite sean ejecutadas en el ambiente.\n",
    "- El tamaño y bordes de las `observaciones` que el ambiente le provee al agente.\n",
    "- Un método *step* para ejecutar un acción. El método regresa la nueva observación, la `recompensa` y la indicación de si el `episodio` ha terminado (*done*). Este método es la interfaz que permite la comunicación entre el ambiente y el agente. Al agente entrega al ambiente una acción seleccionada y este lo retroalimenta a través de este método.\n",
    "- Un método *reset* que retorna al ambiente a su *estado inicial* y entrega la primera observación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d00e5fb-ca34-4653-9f3e-c709d652dd81",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">El espacio de acciones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08157683-e2d3-43ff-8f29-7bd14309fffe",
   "metadata": {},
   "source": [
    "Las acciones que puede realizar un agente puede ser discretas, continuas o incluso una combinación de ambas. Por ejemplo en un automóvil manejado de manera autónoma puede ser posible oprimir varios botones a la vez (discreto), oprimir el pedal del acelerador o del freno (continuo) y así. El espacio de acciones contiene o modela todas las posibilidades de acción del agente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ad16b-e42c-4957-8524-5476670451bd",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">El espacio de observaciones</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007e1305-62f4-4dd5-91a4-fe68ce77191e",
   "metadata": {},
   "source": [
    "Las observaciones son piezas de información que el ambiente puede entregar al agente en cualquier momento del tiempo, además de la recompensa. Las observaciones pueden ser muy simples como un arreglo unidimensional de números o  pueden ser  tensores muy complejos, como las imágenes provenientes de varias cámaras del automóvil.\n",
    "\n",
    "Es claro que acciones y observaciones tienen similaridades, por lo que Gym dispone de una clase abstracta llamada *Space* que permite derivar espacios de acciones y espacios de observaciones. La siguiente imagen muestra una diagrama de la clase *Space*.\n",
    "\n",
    "Existen otras subclases de *Space*, pero estas son las más comúnmente utilizadas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374739ea-141b-4a29-89e1-2ef87bed6b80",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Alejandria/main/Aprendizaje_Reforzado/Imagenes/jerarquia_clase_Space_Gym.png\" width=\"600\" height=\"400\" align=\"center\"/>\n",
    "</center>\n",
    "<center><caption>Jerarquía de la clase Space de OpenAI Gym</caption></center>\n",
    "</figure>\n",
    "\n",
    "Fuente: Alvaro Montenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc147e-8c2f-4028-ad45-aa897b0fd0fb",
   "metadata": {},
   "source": [
    "La clase abstracta *Space* incluye dos métodos relevantes:\n",
    "\n",
    "* *sample()*: retorna una muestra aleatoria del espacio.\n",
    "* *contains(x)*: chequea si el argumento es un elemento del dominio del espacio.\n",
    "\n",
    "Ambos métodos son abstractos y deben ser implementados en las subclases de *Space*.\n",
    "\n",
    "* La clase *Discrete* representa un conjunto de ítems mutuamente excluyentes, numerados entre 0 y $n-1$. Su único atributo *n*, es una cuenta de los ítems que la clase describe. Por ejemplo. *Discrete(n=4)* puede representar por ejemplo un espacio de acciones de cuatro direcciones para moverse: *[izquierda, derecha, arriba, abajo]*.\n",
    "\n",
    "* La clase *Box* representa un tensor *n*-dimensional de números racionales con intervalos *[bajo, alto]*. Por ejemplo puede representar el pedal de un acelerador con un valor simple entre 0.0 y 1.0. En pedal sería codificado en tal caso como *Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)*. Otro ejemplo puede ser la observación de la pantalla en un juego Atari, la cual es RGB de tamaño 210*160: *Box(low=0, high=255, shape=(210, 160, 3), dtype=np.uint8)*.\n",
    "\n",
    "* La última subclase de *Space* es *Tuple*, la cual permite combinar varias instancias de *Space* juntas.Esta subclase permite crear espacios de acciones y observaciones de la complejidad que se desee.\n",
    "\n",
    "Un ejemplo del uso de tupla puede ser el siguiente. Supongamos que deseamos modelar algunos controles de un automóvil. Para empezar la dirección (el ángulo de la dirección, en realidad), el acelerador y el freno puede cambiar en cada instante de tiempo. Por otro lado, podemos tener botones discretos para representar las luces direccionales *(apagadas, izquierda, derecha)* y la bocina *(encendida, apagada)*. Entonces, podemos crear *Tupla(spaces=(Box(low=0.0, high=1.0, shape=(3,), dtype=np.float32), Discrete(3), Discrete(2)\n",
    ")*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921b93b-c965-4151-9465-f48ffc6d15e1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:#4CC9F0\">El ambiente</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68234d15-59fb-48f8-b091-8a19fc0bfc01",
   "metadata": {},
   "source": [
    "El ambiente es representado por la clase *Env*. Todo ambiente tiene dos miembros de tipo *Space*: \n",
    "\n",
    "* *action_space*: acciones permitidas que entrega el ambiente al agente.\n",
    "* *observation_space*: observaciones proveídas por el ambiente al agente.\n",
    "\n",
    "Además el ambiente tiene los siguientes miembros:\n",
    "\n",
    "* *reset()*: coloca el ambiente en el estado inicial.\n",
    "* *step()*: Permite al agente tomar una acción y retorna información como respuesta a la acción: la siguiente observación, la recompensa y la bandera indicando si el episodio ha  finalizado. Esta es la pieza central de la funcionalidad del ambiente.\n",
    "\n",
    "Estas características permiten crear código genérico que podría trabajar con cualquier ambiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d77f45f-bca5-4224-a32e-74486611c682",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Lista de ambientes en OpenAI Gym</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbaf73-c251-4590-b9c0-0ef891d837a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "print(*env_ids, sep=', ...,  ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44710a29-0ae2-46d9-b75f-ec3d7e65c352",
   "metadata": {},
   "source": [
    "## <span style=\"color:blue\">Interactuando con la API de OpenAI Gym</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c821e3-63e4-40ea-bbbe-d89b275327ae",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Exploración preliminar de los objetos de Gym</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24622c-e1ac-44ca-9d61-2438cb53cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006ba095-b37e-4ad1-9068-860716abf5ae",
   "metadata": {},
   "source": [
    "La observación es un arreglo de cuatro posiciones conteniendo respectivamente  *coordenada del centro de masas*, *velocidad*, *ángulo con respecto a la plataforma*, y *velocidad angular*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8707d-6922-4252-b1e6-8ed6ac0f38dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e15bae-165e-4992-8560-cb684c3ca2d7",
   "metadata": {},
   "source": [
    "El espacio de acciones es de tipo *Discrete* y contiene un atributo que puede representar dos elementos 0 y 1, o izquierda y derecha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320e0be1-d0c1-49cf-b3ea-4245fe6182fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2467084-358f-4876-8c20-f0424aae5844",
   "metadata": {},
   "source": [
    "El espacio de acciones es de tipo *Box* conteniendo 4 valores entre los límites mostrado aquí."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f55c33-50a1-4892-a81c-71df5121254c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 0\n",
    "observation, reward, done, info = env.step(action)\n",
    "print('obs: ', np.round(observation,4), end='; ')\n",
    "print('reward: ', reward, end='; ')\n",
    "print('done: ', done, end='; ')\n",
    "print('info: ', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d504cd79-4a99-4268-8720-ec263afb5fda",
   "metadata": {},
   "source": [
    "Con esta sentencia hemos empujado la plataforma a la izquierda (action=0). Como respuesta se ha obtenido un tupla con la siguiente información:\n",
    "\n",
    "* una nueva observación: [ 0.0253, -0.5594,  0.0178,  0.9062].\n",
    "* una recompensa de 1.0,\n",
    "* la bandera *done* con valor *False*, indicando que le episodio aún no termina, y\n",
    "* información extra acerca del ambiente que es un diccionario, que en este caso está vacío."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce4c38f-5eb2-45e5-8435-cf24467e7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f35b1c-bfc8-48f7-90e4-16ea402d44b0",
   "metadata": {},
   "source": [
    "Esta es una muestra aleatoria del espacio de observaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84185d1-7f39-49c7-9754-a40eb0384612",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Interacción con el ambiente de Gym</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106625de-cd40-4643-8d6d-52098cd262a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "# revisemos primero algunos objetos de Gym\n",
    "print('espacio de acciones: ', env.action_space)\n",
    "print('espacio de observaciones: ', env.observation_space)\n",
    "print()\n",
    "\n",
    "# interactuamos con Gym\n",
    "observation = env.reset()\n",
    "print('Comenzamos')\n",
    "print('obs: ', observation)\n",
    "\n",
    "for i in range(100):\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    print(i, ',', end=' ')\n",
    "    print('primera observación: ', np.round(observation,4), end='; ')\n",
    "    print('reward: ', reward, end='; ')\n",
    "    print('done: ', done, end='; ')\n",
    "    print('info: ', info)\n",
    "    if done:\n",
    "        observation = env.reset()\n",
    "env.close()\n",
    "print('Hecho')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec7f62",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:#4361EE\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75ac7d2",
   "metadata": {},
   "source": [
    "1. [OpenAI Gym - Github](https://github.com/openai/gym)\n",
    "1. [Alvaro Montenegro y Daniel Montenegro, Inteligencia Artificial y Aprendizaje Profundo, 2021](https://github.com/AprendizajeProfundo/Diplomado)\n",
    "1. [Maxim Lapan, Deep Reinforcement Learning Hands-On: Apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more, 2nd Edition, 2020](http://library.lol/main/F4D1A90C476A576238E8FE1F47602C67)\n",
    "1. [Richard S. Sutton, Andrew G. Barto, Reinforcement learning: an introduction, 2nd edition, 2020](http://library.lol/main/6502B74CE247C4CD4D4FB54747AD7C7E)\n",
    "1. [Praveen Palanisamy - Hands-On Intelligent Agents with OpenAI Gym_ Your Guide to Developing AI Agents Using Deep Reinforcement Learning, 2020](http://library.lol/main/E4FD128CF9B93E0F7A542B053330517A)\n",
    "1. [Turing Paper 1936](http://www.thocp.net/biographies/papers/turing_oncomputablenumbers_1936.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f08154012ddadd8e950e6e9e035c7a7b32c136e7647e9b7c77e02eb723a8bedb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
