{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#F72585\"><center>Generación de texto usando caracteres y redes recurrentes </center></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Introducción</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Libro-Fundamentos/main/Tratamiento_de_Lenguaje_Natural/Imagenes/maquina_de_escribir.jpg\" width=\"600\" height=\"600\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Muestreo  de palabras</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [Pexels](https://www.pexels.com/es-es/foto/reescribir-y-editar-texto-en-una-maquina-de-escribir-3631711/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Introducción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este cuaderno es una adaptación del tutorial de tensorflow [Text generation with a RNN](https://www.tensorflow.org/tutorials/text/text_generation).\n",
    "\n",
    "Se muestra como generar texto usando una RNR basado en caracteres. Trabajaremos con un conjunto de datos de los algunos documentos de cuentos disponibles en la [biblioteca libre Gutemberg](https://www.gutenberg.org/), con los códigos:  \n",
    "\n",
    "+ 55514-0.txt, \n",
    "+ 61244-0.txt, \n",
    "+ pg36805.txt, \n",
    "+ pg45438.txt, \n",
    "+ pg46000.txt, \n",
    "+ 30053-0.txt,\n",
    "\n",
    "Y los poemas de Daniel Montenegro, uno de los autores:\n",
    "\n",
    "+ Poemas_Output.txt\n",
    "+ Poemas_Todo.txt\n",
    "\n",
    "Los datos fueron preparados por Alvaro Montenegro, uno de los autores.\n",
    "\n",
    "Dada una secuencia de caracteres a partir de estos datos  se entrena un modelo para predecir el siguiente caracter en la secuencia. \n",
    "\n",
    "Se pueden generar secuencias de texto más largas llamando al modelo repetidamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#4361EE\">Importa módulos requeridos</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versión de Tensorflow:  2.8.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "\n",
    "print(\"Versión de Tensorflow: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Lee los datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtiene el `path` en donde están los datos y los lee en una única lista."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../Datos/RNR/61244-0.txt',\n",
       " '../Datos/RNR/55514-0.txt',\n",
       " '../Datos/RNR/pg46000.txt',\n",
       " '../Datos/RNR/Poemas_Output.txt',\n",
       " '../Datos/RNR/Poemas_Todo.txt',\n",
       " '../Datos/RNR/pg45438.txt',\n",
       " '../Datos/RNR/pg36805.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '../Datos/RNR/'\n",
    "files = glob.glob(path+ '*.txt')\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "for file in files:\n",
    "    t = open(files[6], 'rb').read().decode(encoding='utf-8')\n",
    "    text.extend(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1334361"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Una mirada a los primeros 250 caracteres</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\r', '\\n', 'L', 'O', 'S', ' ', 'C', 'O', 'N', 'S', 'E', 'J', 'O', 'S', ' ', 'D', 'E', ' ', 'U', 'N', ' ', 'P', 'A', 'D', 'R', 'E', '\\r', '\\n', '\\r', '\\n', '\\r', '\\n', 'E', 'l', ' ', 'L', 'e', 'ó', 'n', ',', ' ', 'e', 'l', ' ', 'r', 'e', 'y', ' ', 'd', 'e', ' ', 'l', 'a', 's', ' ', 's', 'e', 'l', 'v', 'a', 's', ',', ' ', 'a', 'g', 'o', 'n', 'i', 'z', 'a', 'b', 'a', ' ', 'e', 'n', ' ', 'e', 'l', ' ', 'h', 'u', 'e', 'c', 'o', ' ', 'd', 'e', ' ', 's', 'u', ' ', 'c', 'a', 'v', 'e', 'r', 'n', 'a', '.', '.', '.', '.', '\\r', '\\n', '\\r', '\\n', 'Á', ' ', 's', 'u', ' ', 'l', 'a', 'd', 'o', ' ', 'e', 's', 't', 'a', 'b', 'a', ' ', 's', 'u', ' ', 'h', 'i', 'j', 'o', ',', ' ', 'e', 'l', ' ', '_', 'n', 'u', 'e', 'v', 'o', ' ', 'l', 'e', 'ó', 'n', '_', ',', ' ', 'e', 'l', ' ', 'r', 'e', 'y', ' ', 'f', 'u', 't', 'u', 'r', 'o', ' ', 'd', 'e', ' ', 't', 'o', 'd', 'o', 's', ' ', 'l', 'o', 's', '\\r', '\\n', 'a', 'n', 'i', 'm', 'a', 'l', 'e', 's', '.', '\\r', '\\n', '\\r', '\\n', 'E', 'l', ' ', 'm', 'o', 'n', 'a', 'r', 'c', 'a', ' ', 'm', 'o', 'r', 'i', 'b', 'u', 'n', 'd', 'o', ' ', 'l', 'e', ' ', 'd', 'a', 'b', 'a', ' ', 'p', 'e', 'n', 'o', 's', 'a', 'm', 'e', 'n', 't', 'e', ' ', 'e', 'l', ' ', 'ú', 'l', 't', 'i', 'm', 'o', ' ', 'c', 'o', 'n', 's', 'e', 'j', 'o', ',', ' ']\n"
     ]
    }
   ],
   "source": [
    "# Echa un vistazo a los primeros 250 caracteres del texto\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\r',\n",
       " '\\n',\n",
       " 'L',\n",
       " 'O',\n",
       " 'S',\n",
       " ' ',\n",
       " 'C',\n",
       " 'O',\n",
       " 'N',\n",
       " 'S',\n",
       " 'E',\n",
       " 'J',\n",
       " 'O',\n",
       " 'S',\n",
       " ' ',\n",
       " 'D',\n",
       " 'E',\n",
       " ' ',\n",
       " 'U',\n",
       " 'N',\n",
       " ' ',\n",
       " 'P',\n",
       " 'A',\n",
       " 'D',\n",
       " 'R',\n",
       " 'E',\n",
       " '\\r',\n",
       " '\\n',\n",
       " '\\r',\n",
       " '\\n',\n",
       " '\\r',\n",
       " '\\n',\n",
       " 'E',\n",
       " 'l',\n",
       " ' ',\n",
       " 'L',\n",
       " 'e',\n",
       " 'ó',\n",
       " 'n',\n",
       " ',',\n",
       " ' ',\n",
       " 'e',\n",
       " 'l',\n",
       " ' ',\n",
       " 'r',\n",
       " 'e',\n",
       " 'y',\n",
       " ' ',\n",
       " 'd',\n",
       " 'e']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Crea  el alfabeto</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este alfabeto es una lista que contiene las letras mayúsculas y minúsculas, y algunos caracteres especiales, incluyendo el salto de línea `\\n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 caracteres únicos\n"
     ]
    }
   ],
   "source": [
    "# Los caracteres únicos en el archivo.\n",
    "vocab = sorted(set(text))\n",
    "print ('{} caracteres únicos'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " '\\r',\n",
       " ' ',\n",
       " '!',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'B',\n",
       " 'C',\n",
       " 'D',\n",
       " 'E',\n",
       " 'F',\n",
       " 'G',\n",
       " 'H',\n",
       " 'I',\n",
       " 'J',\n",
       " 'L',\n",
       " 'M',\n",
       " 'N',\n",
       " 'O',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'R',\n",
       " 'S',\n",
       " 'T',\n",
       " 'U',\n",
       " 'V',\n",
       " 'X',\n",
       " 'Y',\n",
       " 'Z',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '¡',\n",
       " '«',\n",
       " '»',\n",
       " '¿',\n",
       " 'Á',\n",
       " 'É',\n",
       " 'Í',\n",
       " 'Ñ',\n",
       " 'Ó',\n",
       " 'á',\n",
       " 'é',\n",
       " 'í',\n",
       " 'ï',\n",
       " 'ñ',\n",
       " 'ó',\n",
       " 'ú',\n",
       " 'ü']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Crea los diccionarios </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Caracter a índice\n",
    "- Índice a caracter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un mapeo de caracteres únicos a índices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " '\\r': 1,\n",
       " ' ': 2,\n",
       " '!': 3,\n",
       " '(': 4,\n",
       " ')': 5,\n",
       " '*': 6,\n",
       " ',': 7,\n",
       " '-': 8,\n",
       " '.': 9,\n",
       " ':': 10,\n",
       " ';': 11,\n",
       " '?': 12,\n",
       " 'A': 13,\n",
       " 'B': 14,\n",
       " 'C': 15,\n",
       " 'D': 16,\n",
       " 'E': 17,\n",
       " 'F': 18,\n",
       " 'G': 19,\n",
       " 'H': 20,\n",
       " 'I': 21,\n",
       " 'J': 22,\n",
       " 'L': 23,\n",
       " 'M': 24,\n",
       " 'N': 25,\n",
       " 'O': 26,\n",
       " 'P': 27,\n",
       " 'Q': 28,\n",
       " 'R': 29,\n",
       " 'S': 30,\n",
       " 'T': 31,\n",
       " 'U': 32,\n",
       " 'V': 33,\n",
       " 'X': 34,\n",
       " 'Y': 35,\n",
       " 'Z': 36,\n",
       " '[': 37,\n",
       " ']': 38,\n",
       " '_': 39,\n",
       " 'a': 40,\n",
       " 'b': 41,\n",
       " 'c': 42,\n",
       " 'd': 43,\n",
       " 'e': 44,\n",
       " 'f': 45,\n",
       " 'g': 46,\n",
       " 'h': 47,\n",
       " 'i': 48,\n",
       " 'j': 49,\n",
       " 'k': 50,\n",
       " 'l': 51,\n",
       " 'm': 52,\n",
       " 'n': 53,\n",
       " 'o': 54,\n",
       " 'p': 55,\n",
       " 'q': 56,\n",
       " 'r': 57,\n",
       " 's': 58,\n",
       " 't': 59,\n",
       " 'u': 60,\n",
       " 'v': 61,\n",
       " 'w': 62,\n",
       " 'x': 63,\n",
       " 'y': 64,\n",
       " 'z': 65,\n",
       " '¡': 66,\n",
       " '«': 67,\n",
       " '»': 68,\n",
       " '¿': 69,\n",
       " 'Á': 70,\n",
       " 'É': 71,\n",
       " 'Í': 72,\n",
       " 'Ñ': 73,\n",
       " 'Ó': 74,\n",
       " 'á': 75,\n",
       " 'é': 76,\n",
       " 'í': 77,\n",
       " 'ï': 78,\n",
       " 'ñ': 79,\n",
       " 'ó': 80,\n",
       " 'ú': 81,\n",
       " 'ü': 82}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\n', '\\r', ' ', '!', '(', ')', '*', ',', '-', '.', ':', ';', '?',\n",
       "       'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'L', 'M', 'N',\n",
       "       'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z', '[', ']',\n",
       "       '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l',\n",
       "       'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y',\n",
       "       'z', '¡', '«', '»', '¿', 'Á', 'É', 'Í', 'Ñ', 'Ó', 'á', 'é', 'í',\n",
       "       'ï', 'ñ', 'ó', 'ú', 'ü'], dtype='<U1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2char "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Transforma el texto en un arreglo de caracteres </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0, 23, ...,  0,  1,  0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  '\\n':   0,\n",
      "  '\\r':   1,\n",
      "  ' ' :   2,\n",
      "  '!' :   3,\n",
      "  '(' :   4,\n",
      "  ')' :   5,\n",
      "  '*' :   6,\n",
      "  ',' :   7,\n",
      "  '-' :   8,\n",
      "  '.' :   9,\n",
      "  ':' :  10,\n",
      "  ';' :  11,\n",
      "  '?' :  12,\n",
      "  'A' :  13,\n",
      "  'B' :  14,\n",
      "  'C' :  15,\n",
      "  'D' :  16,\n",
      "  'E' :  17,\n",
      "  'F' :  18,\n",
      "  'G' :  19,\n",
      "  ...\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Demostración del uso de los diccionarios</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función `repr` convierte el argumento en una expresión imprimible (cuando es posible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\r', '\\n', 'L', 'O', 'S', ' ', 'C', 'O', 'N', 'S', 'E', 'J', 'O'] ---- caracteres mapeados a int ---- > [ 1  0 23 26 30  2 15 26 25 30 17 22 26]\n"
     ]
    }
   ],
   "source": [
    "# Muestre como los primeros 13 caracteres del texto se asignan a números enteros\n",
    "print ('{} ---- caracteres mapeados a int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">La tarea de predicción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dado un caracter, o una secuencia de caracteres, ¿cuál es el próximo caracter más probable? Esta es la tarea en la que vamos a entrenar al modelo. \n",
    "\n",
    "La entrada al modelo será una secuencia de caracteres, y entrenamos al modelo para predecir la salida, el siguiente caracter en cada paso de tiempo.\n",
    "\n",
    "Dado que las RNR mantienen un estado interno que depende de los elementos vistos anteriormente, dados todos los caracteres calculados hasta este momento, ¿cuál es el siguiente caracter?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Creación de datos de entrenamiento y etiquetas  </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El texto se divide  en secuencias de entrenamiento. Cada secuencia de entrada contendrá  una longitud `seq_length` de caracteres del texto.\n",
    "\n",
    "Para cada secuencia de entrada, las etiquetas correspondientes contienen la misma longitud del texto, excepto que se desplaza un caracter a la derecha.\n",
    "\n",
    "Así que primero se divide el texto en trozos de `seq_length + 1`. Por ejemplo, digamos que `seq_length` es `3` y nuestro texto es \"Hola\". La secuencia de entrada sería \"Hol\" y la secuencia de destino  \"ola\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La oración de longitud máxima que queremos para una sola entrada en caracteres\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego se usa la función `tf.data.Dataset.from_tensor_slices` para convertir el vector de texto en una secuencia de índices de caracteres. Un tensor de enteros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "L\n",
      "O\n",
      "S\n",
      " \n",
      "C\n",
      "O\n",
      "N\n",
      "S\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 22:29:25.156492: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-23 22:29:25.179711: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# Crear los ejemplos de entrenamiento / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(10):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset element_spec=TensorSpec(shape=(), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método por lotes nos permite convertir fácilmente estos caracteres individuales en secuencias del tamaño deseado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\r\\nLOS CONSEJOS DE UN PADRE\\r\\n\\r\\n\\r\\nEl León, el rey de las selvas, agonizaba en el hueco de su caverna...'\n",
      "'.\\r\\n\\r\\nÁ su lado estaba su hijo, el _nuevo león_, el rey futuro de todos los\\r\\nanimales.\\r\\n\\r\\nEl monarca m'\n",
      "'oribundo le daba penosamente el último consejo, el más\\r\\nimportante.\\r\\n\\r\\n Huye del hombre le decía: huy'\n",
      "'e siempre; no pretendas luchar con él.\\r\\n\\r\\nEres señor absoluto de los demás animales, no los temas; do'\n",
      "'mínalos,\\r\\ncastígalos, devóralos si tienes hambre.\\r\\n\\r\\nCon todos puedes luchar, á todos puedes vencer; '\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=TensorSpec(shape=(101,), dtype=tf.int64, name=None)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['\\r', '\\n', 'L', 'O', 'S', ' ', 'C', 'O', 'N', 'S', 'E', 'J', 'O',\n",
      "       'S', ' ', 'D', 'E', ' ', 'U', 'N', ' ', 'P', 'A', 'D', 'R', 'E',\n",
      "       '\\r', '\\n', '\\r', '\\n', '\\r', '\\n', 'E', 'l', ' ', 'L', 'e', 'ó',\n",
      "       'n', ',', ' ', 'e', 'l', ' ', 'r', 'e', 'y', ' ', 'd', 'e', ' ',\n",
      "       'l', 'a', 's', ' ', 's', 'e', 'l', 'v', 'a', 's', ',', ' ', 'a',\n",
      "       'g', 'o', 'n', 'i', 'z', 'a', 'b', 'a', ' ', 'e', 'n', ' ', 'e',\n",
      "       'l', ' ', 'h', 'u', 'e', 'c', 'o', ' ', 'd', 'e', ' ', 's', 'u',\n",
      "       ' ', 'c', 'a', 'v', 'e', 'r', 'n', 'a', '.', '.', '.'], dtype='<U1')\n",
      "array(['.', '\\r', '\\n', '\\r', '\\n', 'Á', ' ', 's', 'u', ' ', 'l', 'a',\n",
      "       'd', 'o', ' ', 'e', 's', 't', 'a', 'b', 'a', ' ', 's', 'u', ' ',\n",
      "       'h', 'i', 'j', 'o', ',', ' ', 'e', 'l', ' ', '_', 'n', 'u', 'e',\n",
      "       'v', 'o', ' ', 'l', 'e', 'ó', 'n', '_', ',', ' ', 'e', 'l', ' ',\n",
      "       'r', 'e', 'y', ' ', 'f', 'u', 't', 'u', 'r', 'o', ' ', 'd', 'e',\n",
      "       ' ', 't', 'o', 'd', 'o', 's', ' ', 'l', 'o', 's', '\\r', '\\n', 'a',\n",
      "       'n', 'i', 'm', 'a', 'l', 'e', 's', '.', '\\r', '\\n', '\\r', '\\n',\n",
      "       'E', 'l', ' ', 'm', 'o', 'n', 'a', 'r', 'c', 'a', ' ', 'm'],\n",
      "      dtype='<U1')\n",
      "array(['o', 'r', 'i', 'b', 'u', 'n', 'd', 'o', ' ', 'l', 'e', ' ', 'd',\n",
      "       'a', 'b', 'a', ' ', 'p', 'e', 'n', 'o', 's', 'a', 'm', 'e', 'n',\n",
      "       't', 'e', ' ', 'e', 'l', ' ', 'ú', 'l', 't', 'i', 'm', 'o', ' ',\n",
      "       'c', 'o', 'n', 's', 'e', 'j', 'o', ',', ' ', 'e', 'l', ' ', 'm',\n",
      "       'á', 's', '\\r', '\\n', 'i', 'm', 'p', 'o', 'r', 't', 'a', 'n', 't',\n",
      "       'e', '.', '\\r', '\\n', '\\r', '\\n', ' ', 'H', 'u', 'y', 'e', ' ',\n",
      "       'd', 'e', 'l', ' ', 'h', 'o', 'm', 'b', 'r', 'e', ' ', 'l', 'e',\n",
      "       ' ', 'd', 'e', 'c', 'í', 'a', ':', ' ', 'h', 'u', 'y'], dtype='<U1')\n",
      "array(['e', ' ', 's', 'i', 'e', 'm', 'p', 'r', 'e', ';', ' ', 'n', 'o',\n",
      "       ' ', 'p', 'r', 'e', 't', 'e', 'n', 'd', 'a', 's', ' ', 'l', 'u',\n",
      "       'c', 'h', 'a', 'r', ' ', 'c', 'o', 'n', ' ', 'é', 'l', '.', '\\r',\n",
      "       '\\n', '\\r', '\\n', 'E', 'r', 'e', 's', ' ', 's', 'e', 'ñ', 'o', 'r',\n",
      "       ' ', 'a', 'b', 's', 'o', 'l', 'u', 't', 'o', ' ', 'd', 'e', ' ',\n",
      "       'l', 'o', 's', ' ', 'd', 'e', 'm', 'á', 's', ' ', 'a', 'n', 'i',\n",
      "       'm', 'a', 'l', 'e', 's', ',', ' ', 'n', 'o', ' ', 'l', 'o', 's',\n",
      "       ' ', 't', 'e', 'm', 'a', 's', ';', ' ', 'd', 'o'], dtype='<U1')\n",
      "array(['m', 'í', 'n', 'a', 'l', 'o', 's', ',', '\\r', '\\n', 'c', 'a', 's',\n",
      "       't', 'í', 'g', 'a', 'l', 'o', 's', ',', ' ', 'd', 'e', 'v', 'ó',\n",
      "       'r', 'a', 'l', 'o', 's', ' ', 's', 'i', ' ', 't', 'i', 'e', 'n',\n",
      "       'e', 's', ' ', 'h', 'a', 'm', 'b', 'r', 'e', '.', '\\r', '\\n', '\\r',\n",
      "       '\\n', 'C', 'o', 'n', ' ', 't', 'o', 'd', 'o', 's', ' ', 'p', 'u',\n",
      "       'e', 'd', 'e', 's', ' ', 'l', 'u', 'c', 'h', 'a', 'r', ',', ' ',\n",
      "       'á', ' ', 't', 'o', 'd', 'o', 's', ' ', 'p', 'u', 'e', 'd', 'e',\n",
      "       's', ' ', 'v', 'e', 'n', 'c', 'e', 'r', ';', ' '], dtype='<U1')\n"
     ]
    }
   ],
   "source": [
    "for item in sequences.take(5):\n",
    "  print(repr(idx2char[item.numpy()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Para cada secuencia, duplíquela y cámbiela para formar el texto de entrada y de destino utilizando el método `map` para aplicar una función simple a cada lote. Es  similar a la función apply de R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(100,), dtype=tf.int64, name=None), TensorSpec(shape=(100,), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([ 1,  0, 23, 26, 30,  2, 15, 26, 25, 30, 17, 22, 26, 30,  2, 16, 17,\n",
      "        2, 32, 25,  2, 27, 13, 16, 29, 17,  1,  0,  1,  0,  1,  0, 17, 51,\n",
      "        2, 23, 44, 80, 53,  7,  2, 44, 51,  2, 57, 44, 64,  2, 43, 44,  2,\n",
      "       51, 40, 58,  2, 58, 44, 51, 61, 40, 58,  7,  2, 40, 46, 54, 53, 48,\n",
      "       65, 40, 41, 40,  2, 44, 53,  2, 44, 51,  2, 47, 60, 44, 42, 54,  2,\n",
      "       43, 44,  2, 58, 60,  2, 42, 40, 61, 44, 57, 53, 40,  9,  9])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([ 0, 23, 26, 30,  2, 15, 26, 25, 30, 17, 22, 26, 30,  2, 16, 17,  2,\n",
      "       32, 25,  2, 27, 13, 16, 29, 17,  1,  0,  1,  0,  1,  0, 17, 51,  2,\n",
      "       23, 44, 80, 53,  7,  2, 44, 51,  2, 57, 44, 64,  2, 43, 44,  2, 51,\n",
      "       40, 58,  2, 58, 44, 51, 61, 40, 58,  7,  2, 40, 46, 54, 53, 48, 65,\n",
      "       40, 41, 40,  2, 44, 53,  2, 44, 51,  2, 47, 60, 44, 42, 54,  2, 43,\n",
      "       44,  2, 58, 60,  2, 42, 40, 61, 44, 57, 53, 40,  9,  9,  9])>)\n",
      "(<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([ 9,  1,  0,  1,  0, 70,  2, 58, 60,  2, 51, 40, 43, 54,  2, 44, 58,\n",
      "       59, 40, 41, 40,  2, 58, 60,  2, 47, 48, 49, 54,  7,  2, 44, 51,  2,\n",
      "       39, 53, 60, 44, 61, 54,  2, 51, 44, 80, 53, 39,  7,  2, 44, 51,  2,\n",
      "       57, 44, 64,  2, 45, 60, 59, 60, 57, 54,  2, 43, 44,  2, 59, 54, 43,\n",
      "       54, 58,  2, 51, 54, 58,  1,  0, 40, 53, 48, 52, 40, 51, 44, 58,  9,\n",
      "        1,  0,  1,  0, 17, 51,  2, 52, 54, 53, 40, 57, 42, 40,  2])>, <tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
      "array([ 1,  0,  1,  0, 70,  2, 58, 60,  2, 51, 40, 43, 54,  2, 44, 58, 59,\n",
      "       40, 41, 40,  2, 58, 60,  2, 47, 48, 49, 54,  7,  2, 44, 51,  2, 39,\n",
      "       53, 60, 44, 61, 54,  2, 51, 44, 80, 53, 39,  7,  2, 44, 51,  2, 57,\n",
      "       44, 64,  2, 45, 60, 59, 60, 57, 54,  2, 43, 44,  2, 59, 54, 43, 54,\n",
      "       58,  2, 51, 54, 58,  1,  0, 40, 53, 48, 52, 40, 51, 44, 58,  9,  1,\n",
      "        0,  1,  0, 17, 51,  2, 52, 54, 53, 40, 57, 42, 40,  2, 52])>)\n"
     ]
    }
   ],
   "source": [
    "for item in dataset.take(2):\n",
    "  print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '\\r\\nLOS CONSEJOS DE UN PADRE\\r\\n\\r\\n\\r\\nEl León, el rey de las selvas, agonizaba en el hueco de su caverna..'\n",
      "Target data: '\\nLOS CONSEJOS DE UN PADRE\\r\\n\\r\\n\\r\\nEl León, el rey de las selvas, agonizaba en el hueco de su caverna...'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 1 ('\\r')\n",
      "  expected output: 0 ('\\n')\n",
      "Step    1\n",
      "  input: 0 ('\\n')\n",
      "  expected output: 23 ('L')\n",
      "Step    2\n",
      "  input: 23 ('L')\n",
      "  expected output: 26 ('O')\n",
      "Step    3\n",
      "  input: 26 ('O')\n",
      "  expected output: 30 ('S')\n",
      "Step    4\n",
      "  input: 30 ('S')\n",
      "  expected output: 2 (' ')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Creación de lotes de entrenamiento</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos `tf.data` para dividir el texto en secuencias manejables. Pero antes de introducir estos datos en el modelo, necesitamos mezclar los datos y empaquetarlos en lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tamaño del lote\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Tamaño de búfer para mezclar el conjunto de datos\n",
    "# (la TF data está diseñado para trabajar con secuencias posiblemente infinitas,\n",
    "# para que no intente mezclar toda la secuencia en la memoria. En cambio,\n",
    "# mantiene un búfer en el que mezcla elementos).\n",
    "\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Construcción del Modelo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Use `tf.keras.Sequential` para definir el modelo. Para este sencillo ejemplo, se utilizan tres capas para definir nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamaño del vocabulario en caracteres\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Dimensión del embedding\n",
    "embedding_dim = 256\n",
    "\n",
    "# Número de unidades de RNR\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Libro-Fundamentos/main/Tratamiento_de_Lenguaje_Natural/Imagenes/text_generation_training.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Modelo de generación de texto</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [TensorFlow](https://www.tensorflow.org/text/tutorials/text_generation#build_the_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Prueba del modelo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ahora ejecute el modelo para ver que se comporta como se esperaba.\n",
    "\n",
    "Primero verifique la forma de la salida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 83) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el ejemplo anterior, la longitud de secuencia de la entrada es **100** pero el modelo puede ejecutarse en entradas de cualquier longitud.\n",
    "\n",
    "El dataset de entrenamiento que tiene lotes de tamaño 64 * 100, los cuales tiene orden aleatorio. El primer lote actual y se pasa por el modelo.\n",
    "\n",
    "El modelo tiene de momento los pesos iniciales. El lote de datos pasa por el modelo y devuelve un tensor de tamaño (64, 100, 83).\n",
    "\n",
    "Como el vocabulario tiene 83 caracteres y cada secuencia tiene tamaño 100, el modelo predice el siguiente caracter para cada caracter en la entrada.\n",
    "\n",
    "La predicción funciona así:\n",
    "\n",
    "Por cada caracter de entrada, se predice el siguiente caracter así: el modelo asigna un valor numérico a cada elemento en el vocabulario. Si se eligiera el máximo el caracter seleccionado sería simplemente el caracter con el valor más grande.\n",
    "\n",
    "Ya por fuera del modelo.\n",
    "\n",
    "El siguiente es el vector de predicciones del siguiente caracter para todos los caracteres de la primera secuencia en el primer bloque."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (64, None, 256)           21248     \n",
      "                                                                 \n",
      " gru (GRU)                   (64, None, 1024)          3938304   \n",
      "                                                                 \n",
      " dense (Dense)               (64, None, 83)            85075     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,044,627\n",
      "Trainable params: 4,044,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para obtener predicciones reales del modelo, necesitamos tomar muestras de la distribución de salida, para obtener índices de caracteres reales. Esta distribución está definida por los [logits](https://es.wikipedia.org/wiki/Logit) sobre el vocabulario de los caracteres."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Nota\n",
    "Es mejor muestrear esta distribución, que tomar el `argmax` para evitar que el modelo quede atascado en un bucle. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probemos para el primer ejemplo en el lote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 83), dtype=float32, numpy=\n",
       "array([[ 7.7511012e-03,  2.1465146e-03, -8.4174350e-03, ...,\n",
       "        -2.0824226e-03,  3.4331684e-03, -5.3737313e-07],\n",
       "       [ 2.2362726e-02,  8.3696060e-03, -9.4878953e-03, ...,\n",
       "        -2.8603021e-03,  1.5719300e-03, -6.9782292e-03],\n",
       "       [ 8.5425414e-03,  2.9341828e-03, -6.2398179e-03, ...,\n",
       "         8.0953473e-03,  9.1030542e-04, -2.2933993e-02],\n",
       "       ...,\n",
       "       [ 5.7159332e-03, -1.3102829e-03, -1.2524583e-02, ...,\n",
       "         3.2841796e-03, -6.1105359e-03,  1.9473399e-03],\n",
       "       [ 9.2278738e-03, -5.5279848e-03,  8.8485423e-05, ...,\n",
       "        -1.1623040e-02, -8.1506539e-03,  2.4504704e-02],\n",
       "       [-1.2800302e-03, -1.3847392e-02,  1.2020447e-03, ...,\n",
       "        -1.0831656e-03, -1.2853913e-02,  1.6620940e-02]], dtype=float32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_batch_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos valores son logits para obtener probabilidades. Entonces se normaliza a una distribución categórica, es decir se usa softmax. Con esto, se genera una muestra de la variable categórica para seleccionar el caracter.\n",
    "\n",
    "El siguiente es el vector de predicciones del siguiente caracter para todos los caracteres de  la primera secuencia en el primer bloque.\n",
    "\n",
    "Vamos a predecir los caracteres como indicamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100, 1), dtype=int64, numpy=\n",
       "array([[52],\n",
       "       [64],\n",
       "       [41],\n",
       "       [17],\n",
       "       [11],\n",
       "       [70],\n",
       "       [66],\n",
       "       [16],\n",
       "       [42],\n",
       "       [23],\n",
       "       [20],\n",
       "       [38],\n",
       "       [33],\n",
       "       [16],\n",
       "       [ 8],\n",
       "       [29],\n",
       "       [57],\n",
       "       [21],\n",
       "       [24],\n",
       "       [35],\n",
       "       [56],\n",
       "       [13],\n",
       "       [66],\n",
       "       [59],\n",
       "       [17],\n",
       "       [79],\n",
       "       [72],\n",
       "       [28],\n",
       "       [68],\n",
       "       [51],\n",
       "       [23],\n",
       "       [12],\n",
       "       [17],\n",
       "       [52],\n",
       "       [33],\n",
       "       [66],\n",
       "       [36],\n",
       "       [ 0],\n",
       "       [43],\n",
       "       [20],\n",
       "       [ 9],\n",
       "       [50],\n",
       "       [25],\n",
       "       [45],\n",
       "       [15],\n",
       "       [30],\n",
       "       [68],\n",
       "       [30],\n",
       "       [52],\n",
       "       [10],\n",
       "       [25],\n",
       "       [25],\n",
       "       [36],\n",
       "       [47],\n",
       "       [12],\n",
       "       [53],\n",
       "       [21],\n",
       "       [62],\n",
       "       [24],\n",
       "       [21],\n",
       "       [70],\n",
       "       [14],\n",
       "       [60],\n",
       "       [ 3],\n",
       "       [82],\n",
       "       [ 1],\n",
       "       [77],\n",
       "       [56],\n",
       "       [ 1],\n",
       "       [69],\n",
       "       [48],\n",
       "       [54],\n",
       "       [13],\n",
       "       [73],\n",
       "       [72],\n",
       "       [21],\n",
       "       [74],\n",
       "       [77],\n",
       "       [18],\n",
       "       [62],\n",
       "       [15],\n",
       "       [57],\n",
       "       [47],\n",
       "       [51],\n",
       "       [48],\n",
       "       [58],\n",
       "       [82],\n",
       "       [ 3],\n",
       "       [44],\n",
       "       [43],\n",
       "       [57],\n",
       "       [69],\n",
       "       [ 6],\n",
       "       [ 4],\n",
       "       [34],\n",
       "       [64],\n",
       "       [52],\n",
       "       [49],\n",
       "       [64],\n",
       "       [41]])>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52, 64, 41, 17, 11, 70, 66, 16, 42, 23, 20, 38, 33, 16,  8, 29, 57,\n",
       "       21, 24, 35, 56, 13, 66, 59, 17, 79, 72, 28, 68, 51, 23, 12, 17, 52,\n",
       "       33, 66, 36,  0, 43, 20,  9, 50, 25, 45, 15, 30, 68, 30, 52, 10, 25,\n",
       "       25, 36, 47, 12, 53, 21, 62, 24, 21, 70, 14, 60,  3, 82,  1, 77, 56,\n",
       "        1, 69, 48, 54, 13, 73, 72, 21, 74, 77, 18, 62, 15, 57, 47, 51, 48,\n",
       "       58, 82,  3, 44, 43, 57, 69,  6,  4, 34, 64, 52, 49, 64, 41])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto nos da en cada paso de tiempo, una predicción del siguiente índice de caracteres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([52, 64, 41, 17, 11, 70, 66, 16, 42, 23, 20, 38, 33, 16,  8, 29, 57,\n",
       "       21, 24, 35, 56, 13, 66, 59, 17, 79, 72, 28, 68, 51, 23, 12, 17, 52,\n",
       "       33, 66, 36,  0, 43, 20,  9, 50, 25, 45, 15, 30, 68, 30, 52, 10, 25,\n",
       "       25, 36, 47, 12, 53, 21, 62, 24, 21, 70, 14, 60,  3, 82,  1, 77, 56,\n",
       "        1, 69, 48, 54, 13, 73, 72, 21, 74, 77, 18, 62, 15, 57, 47, 51, 48,\n",
       "       58, 82,  3, 44, 43, 57, 69,  6,  4, 34, 64, 52, 49, 64, 41])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta salida significa que el caracter predicho como el siguiente para el primer caracter es el caracter 21 y así sucesivamente.\n",
    "\n",
    "Los textos lucen así actualmente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decodifíquelos para ver el texto predicho por este modelo no entrenado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " 'del brazo.\\r\\nPero se había reunido demasiada gente á su alrededor, y la autoridad\\r\\ntemió que esto fue'\n",
      "\n",
      "Next Char Predictions: \n",
      " 'mybE;Á¡DcLH]VD-RrIMYqA¡tEñÍQ»lL?EmV¡Z\\ndH.kNfCS»Sm:NNZh?nIwMIÁBu!ü\\ríq\\r¿ioAÑÍIÓíFwCrhlisü!edr¿*(Xymjyb'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Entrenamiento </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto, el problema puede tratarse como un problema de clasificación estándar. Dado el estado RNR anterior, y la entrada en este paso de tiempo, predice la clase del siguiente caracter.\n",
    "\n",
    "Se define una función de pérdida adecuada para este caso. Esta función recibe los targets (*labels*) y las predicciones. La función calcula la distribución a partir de las predicciones y genera para cada caracter un valor predicho. Esto es lo que se necesita para calcular la entropía cruzada en este caso (promedio). Este es el valor retornado. En el ejemplo, como entra una secuencia y se predicen 100 caracteres hay 100 distribuciones para el siguiente caracter. Uno por cada caracter en la entrada. En total hay un lote de 64 secuencias. Luego la función de pérdida se basa en el promedio de  64∗100 = 6400  entropías cruzadas dispersas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Adjuntar un optimizador y una función de pérdida</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función de pérdida estándar `tf.keras.losses.sparse_categorical_crossentropy` funciona en este caso porque se aplica en la última dimensión de las predicciones.\n",
    "\n",
    "Debido a que nuestro modelo devuelve logits, debemos establecer el indicador `from_logits`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 83)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.4193115\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Configure el procedimiento de entrenamiento utilizando el método `tf.keras.Model.compile`. Usaremos `tf.keras.optimizers.Adam` con argumentos predeterminados y la función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Configuración de checkpoints</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilice un `tf.keras.callbacks.ModelCheckpoint` para asegurarse de que los puntos de control se guarden durante el entrenamiento. Se crea un directorio en el cual se guardará los *checkpoints* con el estado del modelo. Los pesos son guardados allí para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio en donde se guardarán los checkpoints\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Nombre de los archivos de los  checkpoint \n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Ejecuta el entrenamiento</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "206/206 [==============================] - 915s 4s/step - loss: 2.5266\n",
      "Epoch 2/10\n",
      "206/206 [==============================] - 779s 4s/step - loss: 1.8629\n",
      "Epoch 3/10\n",
      "206/206 [==============================] - 775s 4s/step - loss: 1.5435\n",
      "Epoch 4/10\n",
      "206/206 [==============================] - 774s 4s/step - loss: 1.2481\n",
      "Epoch 5/10\n",
      "206/206 [==============================] - 774s 4s/step - loss: 0.9411\n",
      "Epoch 6/10\n",
      "206/206 [==============================] - 772s 4s/step - loss: 0.6339\n",
      "Epoch 7/10\n",
      "206/206 [==============================] - 773s 4s/step - loss: 0.4297\n",
      "Epoch 8/10\n",
      "206/206 [==============================] - 776s 4s/step - loss: 0.3425\n",
      "Epoch 9/10\n",
      "206/206 [==============================] - 769s 4s/step - loss: 0.3080\n",
      "Epoch 10/10\n",
      "206/206 [==============================] - 767s 4s/step - loss: 0.2895\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En `history` queda la información de `loss`, y `acurracy` para revisión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Generación de texto </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Restaura el último checkpoint</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección vamos a tomar un preentrenamiento. \n",
    "\n",
    "Usaremos los pesos almacenados, pero ahora vamos a cambiar el modelo para recibir lotes de tamaño 1. Es decir vamos a recibir una secuencia para hacer la predicción a partir de esa secuencia.\n",
    "\n",
    "\n",
    "Debido a la forma en que se pasa el estado RNR de un paso a otro, el modelo solo acepta un tamaño de lote fijo una vez construido.\n",
    "\n",
    "Para ejecutar el modelo con un tamaño de lote diferente, necesitamos reconstruir el modelo y restaurar los pesos desde el punto de control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (1, None, 256)            21248     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (1, None, 1024)           3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 83)             85075     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,044,627\n",
      "Trainable params: 4,044,627\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">El bucle de predicción</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente bloque de código genera el texto:\n",
    "\n",
    "* Comienza eligiendo una cadena de inicio, inicializando el estado de la  RNR y configurando el número de caracteres a generar.\n",
    "\n",
    "* Obtenga la distribución de predicción del siguiente caracter utilizando la cadena de inicio y el estado de la  RNR.\n",
    "\n",
    "* Luego, use una distribución categórica para calcular el índice del caracter predicho. Use este caracter predicho como nuestra próxima entrada al modelo.\n",
    "\n",
    "* El estado de la RNR devuelto por el modelo se retroalimenta al modelo para que ahora tenga más contexto, en lugar de una sola palabra. Después de predecir la siguiente palabra, los estados RNR modificados se retroalimentan nuevamente en el modelo, que es como aprende a medida que obtiene más contexto de las palabras predichas previamente.\n",
    "\n",
    "La siguiente es la línea que define la capa GRU arriba en la definición del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.layers.GRU(rnn_units,  return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro `stateful` determina si los valores de estado recurrente deben mantenerse `True` o no, cuando se pasa al siguiente caracter. En este caso, al mantenerse esos valores, se mantiene la memoria de la secuencia inicial y de los caracteres que van siendo generados. \n",
    "\n",
    "Por eso, solamente se pasa en el primer paso la secuencia de entrada completa. Desde el segundo paso solamente se pasa el nuevo caracter (que es el predicho). Con esta predicción y la memoria del resto de la secuencia anterior mantenida en estado recurrente se hace la siguiente predicción y así sucesivamente tomado la última secuencia pasada por la capa.\n",
    "\n",
    "Para pasar la última secuencia tratada en la capa GRU, se usa el parámetro `return_sequences`. Por defecto es `False`, es decir se omite en la salida la última secuencia pasada por la capa GRU. Aquí hemos colocado `return_sequences=True`, para pasar únicamente el último carater predicho.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Al observar el texto generado, verá que el modelo sabe cuándo colocar mayúsculas, hacer párrafos e imita un vocabulario de escritura similar a Shakespeare. Con el pequeño número de épocas de entrenamiento, aún no ha aprendido a formar oraciones coherentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/AprendizajeProfundo/Libro-Fundamentos/main/Tratamiento_de_Lenguaje_Natural/Imagenes/text_generation_sampling.png\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "</center>\n",
    "<figcaption>\n",
    "<p style=\"text-align:center\">Muestreo  de palabras</p>\n",
    "</figcaption>\n",
    "</figure>\n",
    "\n",
    "Fuente: [TensorFlow](https://www.tensorflow.org/text/tutorials/text_generation#generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función implementa el ciclo de predicción:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Paso de evaluación (generación de texto usando el modelo aprendido)\n",
    "\n",
    "    # Número de caracteres a generar\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Convertir nuestra cadena de inicio en números (vectorizar)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Cadena vacía para almacenar nuestros resultados\n",
    "    text_generated = []\n",
    "\n",
    "    # Las bajas temperaturas dan como resultado un texto más predecible.\n",
    "    # Las temperaturas más altas dan como resultado un texto más sorprendente.\n",
    "    # Experimente para encontrar la mejor configuración.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Aquí batch size == 1\n",
    "    model.reset_states() # borra las unidades recurrentes\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # eliminar la dimensión del lote\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # usar una distribución categórica para predecir la palabra devuelta por el modelo\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # Pasamos la palabra predicha como siguiente entrada al modelo\n",
    "        # junto con el estado oculto anterior\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#4CC9F0\">Notas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `reset_states` borra solo los estados recurrentes de la red. Vale la pena mencionar que dependiendo de si la opción `stateful = True` se configuró en la red, el comportamiento de esta función podría ser diferente. Si no está configurado, todos los estados se restablecen automáticamente después de cada cálculo por lotes en su red (por ejemplo, después de llamar a `fit`, `predict` y `evaluate` también). Si no es así, debe llamar a `reset_states` cada vez que desee que las llamadas del modelo consecutivas sean independientes.\n",
    "2. `tf.expand_dims` devuelve un tensor con una dimensión adicional insertada en el eje del índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO: CASAntr\n",
      "    Un confesonario fino.\n",
      "\n",
      "Dedrándose en partecilla tropezón, se queda otra, llamada María, que es muy amiga del Boti, si el señor\n",
      "Frutos. ¡La\n",
      "noche, que se parece que taña\n",
      "    Una lanchita con repararnos!_\n",
      "\n",
      "An la talde su dese arrastrándose de aquel sitio, pero en dando de Fuencar, única propiedad que no\n",
      "tenía hipotecada. Allí trances altas tornstantes y bienhechores compañeros del oro, que pe\n",
      "acometidad de la niña, y una lluvia y después le cuento en una\n",
      "palamiento.\n",
      "\n",
      " No cabellos casi el blanco ni el fondo de us de un murmitino:\n",
      "\n",
      " Aquéllas que brillaba entre los l Pues, asegurando que el\n",
      "que con ahoras, y\n",
      "lo fintorres lágrimas calicantes una ballena cuando ardon\n",
      "\n",
      "\n",
      "Un hombre de Casilidad, llamado Consejo reflexionaba como un reyobutaban, pagaráje, y como impuso en tono imperativo:\n",
      "\n",
      " ¡Dos billetes de primera para París...!\n",
      "\n",
      "MY oyermás que no han dado las\n",
      "tres, aunque ya faltaron todos á una y otra oreja de la noche.... Si tu misma edad, seis años.... La\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"ROMEO: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo más fácil que puedes hacer para mejorar los resultados es entrenarlo por más tiempo (prueba `EPOCHS = 30`).\n",
    "\n",
    "También puede experimentar con una cadena de inicio diferente, o intentar agregar otra capa RNR para mejorar la precisión del modelo, o ajustar el parámetro de temperatura para generar predicciones más o menos aleatorias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <span style=\"color:#4361EE\">Entrenamiento personalizado </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "El procedimiento de entrenamiento anterior es simple, pero no le da mucho control.\n",
    "\n",
    "Entonces, ahora que ha visto cómo ejecutar el modelo manualmente, descomprimimos el ciclo de entrenamiento e implementémoslo nosotros mismos. Esto proporciona un punto de partida, si por ejemplo, se implementa *aprendizaje curricular* para ayudar a estabilizar la salida de bucle abierto del modelo.\n",
    "\n",
    "Usaremos `tf.GradientTape` para rastrear los gradientes. Puede obtener más información sobre este enfoque leyendo en [diferenciación automática](https://www.tensorflow.org/guide/basics#automatic_differentiation).\n",
    "\n",
    "El procedimiento funciona de la siguiente manera:\n",
    "\n",
    "* Primero, inicialice el estado RNR. Hacemos esto llamando al método `tf.keras.Model.reset_states`.\n",
    "\n",
    "* Luego, repita el conjunto de datos (lote por lote) y calcule las *predicciones* asociadas con cada una.\n",
    "\n",
    "* Abra un `tf.GradientTape`, y calcule las predicciones y pérdidas en ese contexto.\n",
    "\n",
    "* Calcular los gradientes de la pérdida con respecto a las variables del modelo utilizando el método `tf.GradientTape.grads`.\n",
    "\n",
    "* Finalmente, dé un paso hacia abajo utilizando el método `tf.train.Optimizer.apply_gradients` del optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inp)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.keras.losses.sparse_categorical_crossentropy(\n",
    "                target, predictions, from_logits=True))\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 2.0439\n",
      "Time taken for 1 epoch 813.6043629646301 sec\n",
      "\n",
      "Epoch 2 Loss 1.6835\n",
      "Time taken for 1 epoch 814.6451671123505 sec\n",
      "\n",
      "Epoch 3 Loss 1.4132\n",
      "Time taken for 1 epoch 838.6421868801117 sec\n",
      "\n",
      "Epoch 4 Loss 1.1056\n",
      "Time taken for 1 epoch 824.6420001983643 sec\n",
      "\n",
      "Epoch 5 Loss 0.8184\n",
      "Time taken for 1 epoch 749.619856595993 sec\n",
      "\n",
      "Epoch 6 Loss 0.5499\n",
      "Time taken for 1 epoch 751.6419911384583 sec\n",
      "\n",
      "Epoch 7 Loss 0.3963\n",
      "Time taken for 1 epoch 749.1919178962708 sec\n",
      "\n",
      "Epoch 8 Loss 0.3423\n",
      "Time taken for 1 epoch 750.258998632431 sec\n",
      "\n",
      "Epoch 9 Loss 0.3192\n",
      "Time taken for 1 epoch 747.5328431129456 sec\n",
      "\n",
      "Epoch 10 Loss 0.2823\n",
      "Time taken for 1 epoch 830.8155710697174 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paso de entrenamiento\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # inicializando el estado oculto al comienzo de cada época\n",
    "    # inicialmente oculto es Ninguno\n",
    "    hidden = model.reset_states()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "        template = 'Epoch {} Batch {} Loss {}'\n",
    "        print(template.format(epoch+1, batch_n, loss))\n",
    " \n",
    "    # guardar (punto de control) el modelo cada 5 épocas\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   <span style=\"color:#4361EE\">Referencias</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Tensorflow, Text generation with a RNN](https://www.tensorflow.org/tutorials/text/text_generation)\n",
    "1. [Ralf C. Staudemeyer and Eric Rothstein Morris,Understanding LSTM a tutorial into Long Short-Term Memory Recurrent Neural Networks*, arxiv, September 2019](https://arxiv.org/pdf/1909.09586.pdf)\n",
    "1. [Karpathy, The Unreasonable Effectiveness of Recurrent Neural Networks](  http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "1. [Proyecto Gutemberg](https://www.gutenberg.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
